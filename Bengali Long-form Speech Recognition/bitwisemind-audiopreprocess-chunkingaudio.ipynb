{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":129276,"databundleVersionId":15506988,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DEPENDENCY INSTALLATION","metadata":{"id":"xXtXiaZqE6UZ"}},{"cell_type":"code","source":"!pip install -q transformers accelerate pydub librosa rapidfuzz huggingface_hub\n\nprint(\"‚úÖ Dependencies installed.\")","metadata":{"id":"dw22cHBUEew6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMPORTS","metadata":{"id":"mrpet9qiE_c3"}},{"cell_type":"code","source":"import os\nimport re\nimport shutil\nimport zipfile\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom huggingface_hub import HfApi, login\nfrom pydub import AudioSegment\nfrom rapidfuzz import fuzz\nfrom tqdm.auto import tqdm\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\n\nprint(\"‚úÖ Imports ready.\")","metadata":{"id":"jFF5LAPuE815"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFIGURATION","metadata":{"id":"2KBhKu3XFEGP"}},{"cell_type":"markdown","source":"- All paths, model IDs, and processing parameters live here.","metadata":{"id":"-GLaE4bdFGxS"}},{"cell_type":"code","source":"# ‚îÄ‚îÄ File Range ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# Files are named train_001.wav ‚Ä¶ train_124.wav. Set the range to process.\nSTART_FILE = 1    # First file index (inclusive)\nNUM_FILES  = 45     # Number of files to process from START_FILE\n\n# ‚îÄ‚îÄ Dataset Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nAUDIO_BASE_PATH = (\n    \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition\"\n    \"/transcription/transcription/train/audio\"\n)\nTEXT_BASE_PATH = (\n    \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition\"\n    \"/transcription/transcription/train/annotation\"\n)\n\n# ‚îÄ‚îÄ File Naming Patterns ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nAUDIO_FILE_PATTERN = \"train_{:03d}.wav\"\nTEXT_FILE_PATTERN  = \"train_{:03d}.txt\"\n\n# ‚îÄ‚îÄ Output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nOUTPUT_FOLDER = \"./upload_all_25s_1\"\nZIP_FILE_NAME = \"upload_all_25s_1.zip\"\n\n# ‚îÄ‚îÄ HuggingFace Hub ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nHF_TOKEN          = \"\"\nHF_REPO_ID        = \"bitwisemind/preprocess_dataset_25s_chunk\"\nHF_REPO_TYPE      = \"dataset\"\nHF_COMMIT_MESSAGE = \"Upload processed Bengali audio chunks\"\n\n# ‚îÄ‚îÄ Chunking Parameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nCHUNK_MAX_DURATION_S = 25.0\nMIN_CHUNK_DURATION_S = 0.5\n\n# ‚îÄ‚îÄ ASR Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nMODEL_ID = \"bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium\"\nDEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ‚îÄ‚îÄ Clean and recreate output directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nif os.path.exists(OUTPUT_FOLDER):\n    shutil.rmtree(OUTPUT_FOLDER)\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)\n\nprint(\"‚úÖ Configuration loaded.\")\nprint(f\"   Files        : {AUDIO_FILE_PATTERN.format(START_FILE)} ‚Üí \"\n      f\"{AUDIO_FILE_PATTERN.format(START_FILE + NUM_FILES - 1)}\")\nprint(f\"   Chunk size   : {CHUNK_MAX_DURATION_S}s  |  Device : {DEVICE}\")\nprint(f\"   Output       : {OUTPUT_FOLDER}\")\n","metadata":{"id":"afWOcvCMFD8X"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL LOADING","metadata":{"id":"YIsuNratFlre"}},{"cell_type":"markdown","source":"- Loads the BengaliAI Whisper model fine-tuned for Bengali ASR.","metadata":{"id":"76i-R7cdFqUs"}},{"cell_type":"code","source":"print(f\"üì• Loading model : {MODEL_ID}\")\n\nprocessor = WhisperProcessor.from_pretrained(MODEL_ID)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)\nmodel = model.to(DEVICE)\nmodel.eval()\n\nprint(f\"‚úÖ Model ready on {DEVICE}.\")","metadata":{"id":"FVbm-0cOFpyt"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AUDIO CHUNKING & TRANSCRIPTION UTILITIES","metadata":{"id":"8AI9drhOFyCP"}},{"cell_type":"code","source":"def transcribe_chunk(audio_array: np.ndarray, sr: int = 16000) -> str:\n    \"\"\"Transcribe a single 1-D audio array with the Bengali Whisper model.\n\n    The model is already fine-tuned for Bengali; no forced language decoding\n    is needed.\n    \"\"\"\n    input_features = processor(\n        audio_array, sampling_rate=sr, return_tensors=\"pt\"\n    ).input_features.to(DEVICE)\n\n    with torch.no_grad():\n        generated_ids = model.generate(input_features, max_new_tokens=444)\n\n    return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n\ndef chunk_and_transcribe(\n    audio_path: str,\n    max_duration: float = 25.0,\n    output_dir: str = \"./bengali_chunks\",\n) -> List[Dict]:\n    \"\"\"Split a long audio file into chunks, transcribe each, and save WAV files.\n\n    Pipeline per file:\n      1. Load with librosa (for ASR) and pydub (for export).\n      2. Slice into non-overlapping chunks of ‚â§ max_duration seconds.\n      3. Transcribe each chunk; skip silent / empty results.\n      4. Export chunk WAV to output_dir.\n\n    Returns:\n        List of dicts with keys: chunk_id, audio_path, text, start, end, duration.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    name = Path(audio_path).stem\n\n    # Load audio in both formats\n    audio_seg   = AudioSegment.from_file(audio_path)\n    audio_array, sr = librosa.load(audio_path, sr=16000)\n\n    chunk_samples = int(max_duration * sr)\n    total_samples = len(audio_array)\n    num_chunks    = (total_samples + chunk_samples - 1) // chunk_samples\n\n    chunks   = []\n    chunk_id = 0\n\n    for start_sample in tqdm(\n        range(0, total_samples, chunk_samples),\n        desc=f\"  {name}\",\n        total=num_chunks,\n        leave=False,\n    ):\n        end_sample  = min(start_sample + chunk_samples, total_samples)\n        chunk_audio = audio_array[start_sample:end_sample]\n        duration    = len(chunk_audio) / sr\n\n        # Discard near-silent trailing fragments\n        if duration < MIN_CHUNK_DURATION_S:\n            chunk_id += 1\n            continue\n\n        text = transcribe_chunk(chunk_audio, sr)\n\n        # Skip chunks that produced no text (silence / noise)\n        if not text.strip():\n            chunk_id += 1\n            continue\n\n        # Export WAV via pydub (millisecond indexing)\n        start_ms  = int(start_sample / sr * 1000)\n        end_ms    = int(end_sample   / sr * 1000)\n        out_path  = os.path.join(output_dir, f\"{name}_{chunk_id:03d}.wav\")\n        audio_seg[start_ms:end_ms].export(out_path, format=\"wav\")\n\n        chunks.append({\n            \"chunk_id\":   chunk_id,\n            \"audio_path\": out_path,\n            \"text\":       text,\n            \"start\":      round(start_sample / sr, 3),\n            \"end\":        round(end_sample   / sr, 3),\n            \"duration\":   round(duration, 3),\n        })\n        chunk_id += 1\n\n    return chunks\n\n\nprint(\"‚úÖ Chunking and transcription utilities ready.\")","metadata":{"id":"g7x9owrWFlf4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PROCESS ALL FILES","metadata":{"id":"bNEl59JGF8Ca"}},{"cell_type":"markdown","source":"- Iterates over the configured file range, chunks each audio file and accumulates all chunks into a single DataFrame.","metadata":{"id":"sGq7Ch7-GAMh"}},{"cell_type":"code","source":"all_chunks = []\nend_file   = START_FILE + NUM_FILES   # exclusive upper bound\n\nprint(f\"üöÄ Processing files {START_FILE} ‚Üí {end_file - 1}...\\n\")\n\nfor file_num in range(START_FILE, end_file):\n    audio_path = os.path.join(AUDIO_BASE_PATH, AUDIO_FILE_PATTERN.format(file_num))\n    text_path  = os.path.join(TEXT_BASE_PATH,  TEXT_FILE_PATTERN.format(file_num))\n\n    if not os.path.exists(audio_path):\n        print(f\"  ‚ö†Ô∏è  Audio not found: {AUDIO_FILE_PATTERN.format(file_num)} ‚Äî skipped.\")\n        continue\n    if not os.path.exists(text_path):\n        print(f\"  ‚ö†Ô∏è  Annotation not found: {TEXT_FILE_PATTERN.format(file_num)} ‚Äî skipped.\")\n        continue\n\n    print(f\"[{file_num}] {AUDIO_FILE_PATTERN.format(file_num)}\")\n    chunks = chunk_and_transcribe(audio_path, max_duration=CHUNK_MAX_DURATION_S,\n                                  output_dir=OUTPUT_FOLDER)\n\n    for chunk in chunks:\n        chunk[\"file_num\"]  = file_num\n        chunk[\"text_path\"] = text_path\n\n    all_chunks.extend(chunks)\n    print(f\"      ‚Üí {len(chunks)} chunks extracted.\")\n\ndf = pd.DataFrame(all_chunks)\n\n# Save a full manifest (all metadata) before alignment\nmanifest_path = os.path.join(OUTPUT_FOLDER, \"chunks_manifest.csv\")\ndf.to_csv(manifest_path, index=False, encoding=\"utf-8-sig\")\n\nprint(f\"\\n‚úÖ Total chunks : {len(df)}\")\nprint(f\"   Manifest saved ‚Üí {manifest_path}\")\ndf.head(10)","metadata":{"id":"SG7bbSyZF4cx"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GROUND-TRUTH ALIGNMENT","metadata":{"id":"2QjJKijHGNYL"}},{"cell_type":"markdown","source":"- Aligns each ASR-transcribed chunk to the corresponding segment of the ground-truth annotation using sequential fuzzy matching.","metadata":{"id":"RIvUuXU9GR7i"}},{"cell_type":"code","source":"# ‚îÄ‚îÄ Text Cleaning Helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef clean_text(text: str) -> str:\n    \"\"\"Strip Bengali punctuation to normalize text for fuzzy comparison.\"\"\"\n    return re.sub(r\"[‡•§,\\-\\.?!:;\\\"'()]\", \"\", text).strip()\n\n\n# ‚îÄ‚îÄ Alignment Function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef align_chunks_sequential(file_df: pd.DataFrame, corpus_path: str) -> pd.DataFrame:\n    \"\"\"Map each ASR chunk to the best-matching span of the ground-truth corpus.\n\n    Alignment strategy:\n      - Maintains a sequential pointer (corpus_pos) so each chunk maps to the\n        *next unused* portion of the corpus ‚Äî preserving reading order.\n      - A small bidirectional search window (¬±5 words) allows for minor ASR\n        insertions or deletions without de-syncing the pointer.\n      - Fuzzy ratio (RapidFuzz) selects the best candidate span length.\n\n    Args:\n        file_df    : DataFrame of chunks belonging to a single audio file.\n        corpus_path: Path to the plain-text ground-truth annotation file.\n\n    Returns:\n        file_df with a new 'gt' column containing the aligned ground-truth text.\n    \"\"\"\n    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n        corpus = f.read().strip()\n\n    corpus_words = clean_text(corpus).split()\n    gt_texts     = []\n    corpus_pos   = 0   # Sequential word pointer\n\n    for _, row in file_df.iterrows():\n        asr_clean  = clean_text(row[\"text\"])\n        asr_words  = asr_clean.split()\n\n        if not asr_words or corpus_pos >= len(corpus_words):\n            gt_texts.append(\"\")\n            continue\n\n        num_asr_words = len(asr_words)\n        best_score, best_length, best_start = 0, num_asr_words, corpus_pos\n\n        # Search over a small window of start offsets and span lengths\n        for start_offset in range(-5, 10):\n            start = max(0, corpus_pos + start_offset)\n            for length_offset in range(-3, 5):\n                length    = max(5, num_asr_words + length_offset)\n                end       = min(start + length, len(corpus_words))\n                if start >= end:\n                    continue\n                candidate = \" \".join(corpus_words[start:end])\n                score     = fuzz.ratio(asr_clean, candidate)\n                if score > best_score:\n                    best_score  = score\n                    best_length = end - start\n                    best_start  = start\n\n        gt_end  = min(best_start + best_length, len(corpus_words))\n        gt_text = \" \".join(corpus_words[best_start:gt_end])\n        gt_texts.append(gt_text)\n        corpus_pos = gt_end   # Advance pointer past the matched span\n\n    file_df = file_df.copy()\n    file_df[\"gt\"] = gt_texts\n    return file_df\n\n\n# ‚îÄ‚îÄ Run Alignment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nprint(\"üîç Aligning chunks with ground-truth annotations...\\n\")\n\naligned_dfs = []\n\nfor file_num in tqdm(range(START_FILE, end_file), desc=\"Aligning files\"):\n    file_chunks = df[df[\"file_num\"] == file_num].copy()\n    if file_chunks.empty:\n        continue\n\n    text_path = file_chunks.iloc[0][\"text_path\"]\n    if not os.path.exists(text_path):\n        print(f\"  ‚ö†Ô∏è  Annotation missing for file {file_num} ‚Äî skipped.\")\n        continue\n\n    aligned_dfs.append(align_chunks_sequential(file_chunks, text_path))\n\n# Combine, filter empty GT rows, and retain only the columns needed for training\ndf_aligned = (\n    pd.concat(aligned_dfs, ignore_index=True)\n      .pipe(lambda d: d[d[\"gt\"].str.len() > 0])\n      [[\"audio_path\", \"gt\"]]\n)\n\naligned_path = os.path.join(OUTPUT_FOLDER, \"aligned_chunks.csv\")\ndf_aligned.to_csv(aligned_path, index=False, encoding=\"utf-8-sig\")\n\nprint(f\"‚úÖ Aligned chunks : {len(df_aligned)}\")\nprint(f\"   Saved ‚Üí {aligned_path}\")\ndf_aligned.head(10)","metadata":{"id":"y52DKuZQGNOs"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ZIP PACKAGING","metadata":{"id":"IS2jbyxfGebF"}},{"cell_type":"markdown","source":"- Compresses the entire output folder (WAVs + CSVs) into a single ZIP archive ready for upload.","metadata":{"id":"x43HIEwJGiUT"}},{"cell_type":"code","source":"print(f\"üì¶ Packaging output folder ‚Üí {ZIP_FILE_NAME}\")\n\nwith zipfile.ZipFile(ZIP_FILE_NAME, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUTPUT_FOLDER):\n        for file in files:\n            full_path = os.path.join(root, file)\n            arc_name  = os.path.relpath(full_path, os.path.dirname(OUTPUT_FOLDER))\n            zf.write(full_path, arc_name)\n\nzip_size_mb = os.path.getsize(ZIP_FILE_NAME) / (1024 ** 2)\nprint(f\"‚úÖ ZIP ready : {ZIP_FILE_NAME}  ({zip_size_mb:.2f} MB)\")","metadata":{"id":"h0COrXulGc8m"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HUGGINGFACE HUB UPLOAD","metadata":{"id":"nMT6AvfRGush"}},{"cell_type":"markdown","source":"- Authenticates and uploads the ZIP to the configured dataset repo.","metadata":{"id":"L9nKWG0NGyal"}},{"cell_type":"code","source":"if not HF_TOKEN:\n    print(\"‚ö†Ô∏è  HF_TOKEN not set ‚Äî upload skipped.\")\nelse:\n    login(token=HF_TOKEN, add_to_git_credential=False)\n\n    print(f\"üì§ Uploading {ZIP_FILE_NAME} ‚Üí {HF_REPO_ID}\")\n    HfApi().upload_file(\n        path_or_fileobj = ZIP_FILE_NAME,\n        path_in_repo    = ZIP_FILE_NAME,\n        repo_id         = HF_REPO_ID,\n        repo_type       = HF_REPO_TYPE,\n        commit_message  = HF_COMMIT_MESSAGE,\n    )\n    print(f\"‚úÖ Upload complete.\")\n    print(f\"   View at : https://huggingface.co/datasets/{HF_REPO_ID}\")","metadata":{"id":"9zy4dBidGukK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SUMMARY","metadata":{"id":"Pnz0ME6_G6S2"}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"üìä  PIPELINE COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"   Files processed  : {NUM_FILES}\")\nprint(f\"   Total chunks     : {len(df)}\")\nprint(f\"   Aligned chunks   : {len(df_aligned)}\")\nprint(f\"   Output folder    : {OUTPUT_FOLDER}\")\nprint(f\"   ZIP archive      : {ZIP_FILE_NAME}  ({zip_size_mb:.2f} MB)\")\nprint(f\"   HuggingFace repo : {HF_REPO_ID}\")\nprint(\"=\" * 60)","metadata":{"id":"9yb2nB5eG3ig"},"outputs":[],"execution_count":null}]}