{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14746546,"datasetId":9424696,"databundleVersionId":15596114}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DEPENDENCY INSTALLATION    ","metadata":{"id":"AvGOxrPoxqJD"}},{"cell_type":"code","source":"!pip install -q transformers==4.45.0\n!pip install -q datasets==2.16.1\n!pip install -q accelerate==1.0.0\n!pip install -q evaluate==0.4.3\n!pip install -q jiwer==3.0.3\n!pip install -q tensorboard soundfile librosa\n!pip install -q \"bitsandbytes>=0.43.0\"\n!pip install -q num2words bangla huggingface_hub\n\nprint(\"âœ… All dependencies installed.\")","metadata":{"id":"waHsXl_sxjhy"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ENVIRONMENT SETUP & CORE IMPORTS  ","metadata":{"id":"IpYVYjRnxvxt"}},{"cell_type":"code","source":"import os, gc, random, glob, re, warnings, logging\nimport numpy as np\nimport pandas as pd\nimport torch\nimport librosa\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\nwarnings.filterwarnings(\"ignore\")\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]    = \"0\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TRANSFORMERS_VERBOSITY\"]  = \"warning\"\n\nfrom transformers import (\n    WhisperFeatureExtractor,\n    WhisperTokenizer,\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    TrainerCallback,\n)\nimport evaluate\nfrom datasets import Dataset, DatasetDict, Audio\nfrom huggingface_hub import HfApi, login, snapshot_download\n\n\nimport torch.serialization\ntry:\n    torch.serialization.add_safe_globals(\n        [np.ndarray, np.dtype, np.core.multiarray._reconstruct]\n    )\nexcept AttributeError:\n    try:\n        torch.serialization.add_safe_globals(\n            [np.ndarray, np.dtype, np._core.multiarray._reconstruct]\n        )\n    except Exception as e:\n        print(f\"âš ï¸  numpy safe globals registration skipped: {e}\")\n\n_orig_load = torch.load\ndef _patched_load(*args, **kwargs):\n    kwargs.setdefault(\"weights_only\", False)\n    return _orig_load(*args, **kwargs)\ntorch.load = _patched_load\n\nlogging.basicConfig(\n    format  = \"%(asctime)s | %(levelname)s | %(message)s\",\n    datefmt = \"%H:%M:%S\",\n    level   = logging.WARNING,\n)\n\n# â”€â”€ Device Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"ğŸ–¥ï¸  GPU  : {torch.cuda.get_device_name(0)}  |  VRAM : {vram:.1f} GB\")\n    print(f\"   Visible GPU count : {torch.cuda.device_count()}  (must be 1)\")\n    torch.cuda.empty_cache()\nelse:\n    print(\"âš ï¸  Running on CPU â€” training will be very slow.\")\n\ngc.collect()\nprint(\"âœ… Environment ready.\")","metadata":{"id":"ZyAkZDh-xuN5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PROJECT CONFIGURATION","metadata":{"id":"QL0PeEMdyRkW"}},{"cell_type":"markdown","source":"- Edit all paths and hyperparameters here before running","metadata":{"id":"-MgeALXA3qI-"}},{"cell_type":"code","source":"# â”€â”€ Model & HuggingFace Hub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMODEL_NAME     = \"bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium\"\nHF_TOKEN       = \"\"\nHF_OUTPUT_REPO = \"bitwisemind/sam_15000_clean_text_full_model\"\nUPLOAD_CHECKPOINTS = True   # Set False to skip HuggingFace uploads\n\n# â”€â”€ Dataset Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMETADATA_PATH   = \"/kaggle/input/datasets/sazzadhossainsazzad/1-to-121-audio-25s-zip/annotation.csv\"\nAUDIO_BASE_PATH = \"/kaggle/input/datasets/sazzadhossainsazzad/1-to-121-audio-25s-zip/\"\nDATA_START      = 1\nDATA_END        = 15229\nTRAIN_SPLIT     = 0.9     # 90% train / 10% validation\nOUTPUT_PATH     = \"./whisper-fullweight-output\"\n\n# â”€â”€ Training Hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nPER_DEVICE_TRAIN_BATCH = 8   # Per-GPU batch size\nGRAD_ACCUM_STEPS       = 2   # Effective batch = 8 Ã— 2 = 16\nPER_DEVICE_EVAL_BATCH  = 8\nLEARNING_RATE          = 5e-6\nWARMUP_STEPS           = 100\nTARGET_EPOCHS          = 6\n\n# â”€â”€ Audio / Text Constraints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMAX_AUDIO_DURATION = 30\nMAX_TEXT_LENGTH    = 1000\n\n# â”€â”€ Data Augmentation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nUSE_AUGMENTATION   = True\nAUGMENTATION_PROB  = 0.3    # Probability of augmenting a training sample\n\nprint(\"âœ… Configuration loaded.\")\nprint(f\"   Model            : {MODEL_NAME}\")\nprint(f\"   Data range       : rows {DATA_START}â€“{DATA_END}\")\nprint(f\"   Effective batch  : {PER_DEVICE_TRAIN_BATCH * GRAD_ACCUM_STEPS}\")\nprint(f\"   Learning rate    : {LEARNING_RATE}  |  Epochs : {TARGET_EPOCHS}\")\n\n","metadata":{"id":"Kp1gKEzoyRZ9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HUGGINGFACE LOGIN & COMPATIBILITY SHIM    ","metadata":{"id":"kjSUhPsfzL3B"}},{"cell_type":"code","source":"import huggingface_hub.hf_api as _hf_api_module\n\nclass _HfFolderShim:\n    \"\"\"Minimal replacement for the deprecated huggingface_hub.HfFolder.\"\"\"\n    _token = HF_TOKEN\n\n    @classmethod\n    def get_token(cls):   return cls._token\n    @classmethod\n    def save_token(cls, token): cls._token = token\n    @classmethod\n    def delete_token(cls): cls._token = None\n\nif not hasattr(_hf_api_module, \"HfFolder\"):\n    _hf_api_module.HfFolder = _HfFolderShim\n\n# â”€â”€ Authenticate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif HF_TOKEN:\n    login(token=HF_TOKEN, add_to_git_credential=False)\n    print(\"âœ… HuggingFace login successful.\")\nelse:\n    print(\"âš ï¸  No HF_TOKEN provided â€” uploads will be skipped.\")\n","metadata":{"id":"c6FsBlzBzNhK"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHECKPOINT DISCOVERY (LOCAL & REMOTE)","metadata":{"id":"L3YMQV5wzXfO"}},{"cell_type":"markdown","source":"- Downloads an existing checkpoint from HF Hub if one is present, then scans the local output directory for the latest step.","metadata":{"id":"z9adygIe3ipB"}},{"cell_type":"code","source":"def download_checkpoint_from_hf(repo_id: str, local_dir: str, token: str) -> None:\n    \"\"\"Pull the latest checkpoint from a HuggingFace dataset repo.\"\"\"\n    if os.path.exists(local_dir) and os.listdir(local_dir):\n        print(f\"â„¹ï¸  {local_dir} already populated â€” skipping HF download.\")\n        return\n    print(f\"ğŸ“¥ Downloading checkpoint from HuggingFace: {repo_id}\")\n    try:\n        snapshot_download(\n            repo_id=repo_id, repo_type=\"dataset\",\n            local_dir=local_dir, token=token,\n        )\n        print(f\"âœ… Checkpoint downloaded â†’ {local_dir}\")\n    except Exception as e:\n        print(f\"â„¹ï¸  No remote checkpoint found (first run or empty repo): {e}\")\n\n\ndef find_latest_checkpoint(output_dir: str):\n    \"\"\"Return the path to the highest-numbered checkpoint folder, or None.\"\"\"\n    if not os.path.exists(output_dir):\n        return None\n    folders = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n    candidates = []\n    for f in folders:\n        m = re.search(r\"checkpoint-(\\d+)\", f)\n        if m:\n            candidates.append((int(m.group(1)), f))\n    return max(candidates, key=lambda x: x[0])[1] if candidates else None\n\n\ndownload_checkpoint_from_hf(HF_OUTPUT_REPO, OUTPUT_PATH, HF_TOKEN)\n\nRESUME_FROM_CHECKPOINT = find_latest_checkpoint(OUTPUT_PATH)\n\nif RESUME_FROM_CHECKPOINT:\n    print(f\"âœ… Resuming from checkpoint : {RESUME_FROM_CHECKPOINT}\")\nelse:\n    print(\"â„¹ï¸  No checkpoint found â€” training will start from scratch.\")\n\n","metadata":{"id":"cTu6Zrz2zUOq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL & PROCESSOR LOADING","metadata":{"id":"TFR4gkSuz8Wi"}},{"cell_type":"markdown","source":"- Loads the Whisper model in float32 (no quantization).Gradient checkpointing is enabled to reduce VRAM usage.","metadata":{"id":"EWynvV-p3b7w"}},{"cell_type":"code","source":"torch.cuda.empty_cache(); gc.collect()\n\nprint(f\"ğŸ“¦ Loading model : {MODEL_NAME}\")\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n\n# Disable KV cache (incompatible with gradient checkpointing during training)\nmodel.config.use_cache = False\nmodel.gradient_checkpointing_enable()\n\n# Ensure all parameters are trainable (full fine-tuning, no freezing)\nfor param in model.parameters():\n    param.requires_grad = True\n\n# â”€â”€ Parameter Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntotal_params     = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"   Total parameters     : {total_params:,}\")\nprint(f\"   Trainable parameters : {trainable_params:,}  ({100 * trainable_params / total_params:.1f}%)\")\n\nif device == \"cuda\":\n    used  = torch.cuda.memory_allocated() / 1024**3\n    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"   VRAM after loading   : {used:.2f} / {total:.2f} GB  ({total - used:.2f} GB free)\")\n\n# â”€â”€ Processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\ntokenizer  = WhisperTokenizer.from_pretrained(MODEL_NAME, language=\"Bengali\", task=\"transcribe\")\nprocessor  = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"Bengali\", task=\"transcribe\")\n\nprint(\"âœ… Model and processor loaded.\")","metadata":{"id":"9g3D4ZTr0Eu2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TEXT NORMALIZATION & CLEANING UTILITIES","metadata":{"id":"g5u1_ceN0L9G"}},{"cell_type":"code","source":"from num2words import num2words\n\n# â”€â”€ Number-to-Words Conversion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _to_bangla_words(n: int, is_year: bool = False) -> str:\n    \"\"\"Convert an integer to its Bangla word representation.\"\"\"\n    try:\n        return num2words(n, lang=\"bn\", to=\"year\") if is_year else num2words(n, lang=\"bn\")\n    except Exception:\n        return num2words(n, lang=\"bn\")\n\n\ndef convert_numbers_to_bangla(text: str) -> str:\n    \"\"\"Replace all English digit sequences in text with Bangla words.\n\n    4-digit numbers in the range 1000â€“2099 are treated as calendar years.\n    \"\"\"\n    def _replace(m):\n        token = m.group(0)\n        n = int(token)\n        return _to_bangla_words(n, is_year=(len(token) == 4 and 1000 <= n <= 2099))\n    return re.sub(r\"\\d+\", _replace, text)\n\n\ndef normalize_text(text: str) -> str:\n    \"\"\"Normalize a transcript: strip whitespace and expand digits to Bangla words.\"\"\"\n    if pd.isna(text) or not text:\n        return \"\"\n    text = \" \".join(str(text).strip().split())\n    return convert_numbers_to_bangla(text)\n\n\n# â”€â”€ Bangla Character Filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef _is_bangla(char: str) -> bool:\n    return 0x0980 <= ord(char) <= 0x09FF\n\n\ndef _is_allowed(char: str) -> bool:\n    \"\"\"Return True if the character is permitted in a clean Bengali transcript.\"\"\"\n    return (\n        _is_bangla(char)\n        or char in \" \\n\\r\\t\"\n        or char.isdigit()\n        or char in \"à¥¤.,?!-â€”:;\\\"'()[]{}\"\n    )\n\n\ndef clean_annotation_text(text: str):\n    \"\"\"Remove non-Bengali / non-allowed characters from a transcript string.\n\n    Returns:\n        cleaned_text (str), chars_removed (int), had_garbage (bool)\n    \"\"\"\n    cleaned = [c for c in text if _is_allowed(c)]\n    removed = len(text) - len(cleaned)\n    return \"\".join(cleaned), removed, removed > 0\n\n\nprint(\"âœ… Text normalization utilities ready.\")","metadata":{"id":"KedUqkUV0NLg"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AUDIO AUGMENTATION PIPELINE","metadata":{"id":"KwFUf16J0VCG"}},{"cell_type":"markdown","source":"- Applies realistic degradations to a random subset of each audio clip to improve model robustness on noisy, real-world speech.  ","metadata":{"id":"akiEcRmN3V-D"}},{"cell_type":"code","source":"from scipy import signal\nfrom scipy.signal import butter, lfilter\n\n# â”€â”€ Individual Augmentation Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef bandpass_filter(audio: np.ndarray, sr: int,\n                    lowcut: int = 300, highcut: int = 3400) -> np.ndarray:\n    \"\"\"Simulate telephone-quality audio by bandpass filtering.\"\"\"\n    nyq  = sr / 2\n    low  = max(0.001, min(lowcut  / nyq, 0.999))\n    high = max(0.001, min(highcut / nyq, 0.999))\n    if low >= high:\n        return audio\n    b, a = butter(4, [low, high], btype=\"band\")\n    return lfilter(b, a, audio).astype(np.float32)\n\n\ndef build_rir(sr: int, room_size: str = \"medium\") -> np.ndarray:\n    \"\"\"Construct a synthetic Room Impulse Response for convolution reverb.\"\"\"\n    configs = {\n        \"small\":  {\"rt60\": 0.15, \"n_ref\": 6,  \"max_delay\": 0.05},\n        \"medium\": {\"rt60\": 0.40, \"n_ref\": 10, \"max_delay\": 0.12},\n        \"large\":  {\"rt60\": 0.90, \"n_ref\": 16, \"max_delay\": 0.25},\n    }\n    cfg     = configs[room_size]\n    rt60    = cfg[\"rt60\"]\n    rir_len = int(rt60 * sr * 2)\n    rir     = np.zeros(rir_len)\n\n    for _ in range(cfg[\"n_ref\"]):\n        delay = int(random.uniform(0.002, cfg[\"max_delay\"]) * sr)\n        if delay < rir_len:\n            rir[delay] += random.uniform(0.3, 0.9)\n\n    tail_start = int(0.02 * sr)\n    tail = (np.exp(-np.linspace(0, 6.9 / rt60, rir_len - tail_start))\n            * np.random.randn(rir_len - tail_start) * 0.5)\n    rir[tail_start:] += tail\n    mx = np.abs(rir).max()\n    return (rir / mx if mx > 0 else rir).astype(np.float32)\n\n\ndef apply_reverb(segment: np.ndarray, sr: int, room_size: str = None) -> np.ndarray:\n    \"\"\"Convolve a segment with a synthetic room impulse response.\"\"\"\n    if room_size is None:\n        room_size = random.choice([\"small\", \"medium\", \"large\"])\n    rir = build_rir(sr, room_size)\n    wet = signal.fftconvolve(segment, rir, mode=\"full\")[:len(segment)]\n    mx  = np.abs(wet).max()\n    if mx > 0:\n        wet /= mx\n    mix = random.uniform(0.4, 0.85)\n    return ((1 - mix) * segment + mix * wet).astype(np.float32)\n\n\ndef apply_echo(segment: np.ndarray, sr: int) -> np.ndarray:\n    \"\"\"Add multi-tap echo at random delay and decay settings.\"\"\"\n    result        = segment.copy()\n    n_taps        = random.randint(2, 4)\n    base_delay_ms = random.randint(150, 800)\n    decay         = random.uniform(0.4, 0.75)\n    for tap in range(1, n_taps + 1):\n        delay_samples = int((base_delay_ms * tap) * sr / 1000)\n        if delay_samples < len(result):\n            echo = np.zeros_like(result)\n            echo[delay_samples:] = result[:-delay_samples] * (decay ** tap)\n            result = result + echo\n    return result.astype(np.float32)\n\n\ndef apply_noise(segment: np.ndarray, sr: int) -> np.ndarray:\n    \"\"\"Mix pink and white noise into the segment at a low SNR.\"\"\"\n    white = np.random.randn(len(segment))\n    fft   = np.fft.rfft(white)\n    freqs = np.fft.rfftfreq(len(white)); freqs[0] = 1e-6\n    pink  = np.fft.irfft(fft / np.sqrt(freqs), n=len(white))\n    mx    = np.abs(pink).max()\n    if mx > 0:\n        pink /= mx\n    noise_factor = random.uniform(0.02, 0.08)\n    mix          = random.uniform(0.3, 0.7)\n    return (segment + noise_factor * (mix * white + (1 - mix) * pink)).astype(np.float32)\n\n\ndef apply_clipping(segment: np.ndarray, threshold: float = None) -> np.ndarray:\n    \"\"\"Simulate ADC clipping distortion.\"\"\"\n    if threshold is None:\n        threshold = random.uniform(0.4, 0.75)\n    return np.clip(segment, -threshold, threshold).astype(np.float32)\n\n\n# â”€â”€ Main Augmentation Orchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef augment_audio_segments(audio_array: np.ndarray, sr: int = 16000,\n                            augment_percentage: float = 0.30) -> np.ndarray:\n    \"\"\"Apply random augmentations to non-overlapping segments of an audio clip.\n\n    Augments up to `augment_percentage` of the total duration by selecting\n    random 3â€“6 second windows and applying a stochastic chain of effects\n    (noise, echo, reverb, clipping, pitch shift, time stretch).\n    \"\"\"\n    augmented     = audio_array.copy().astype(np.float32)\n    aug_budget    = len(augmented) / sr * augment_percentage\n    used_segments = []\n    total_aug_time = 0.0\n\n    while total_aug_time < aug_budget:\n        seg_dur     = min(random.uniform(3, 6), aug_budget - total_aug_time)\n        seg_samples = int(seg_dur * sr)\n        if seg_samples <= 0:\n            break\n\n        # Find a non-overlapping start position\n        start = None\n        for _ in range(60):\n            if len(augmented) <= seg_samples:\n                break\n            candidate = random.randint(0, len(augmented) - seg_samples)\n            end       = candidate + seg_samples\n            if all(end <= s or candidate >= e for s, e in used_segments):\n                start = candidate\n                break\n\n        if start is None:\n            break\n\n        end = start + seg_samples\n        used_segments.append((start, end))\n        seg = augmented[start:end].copy()\n\n        # â”€â”€ Stochastic effect chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        if random.random() < 0.65: seg = apply_noise(seg, sr)\n        if random.random() < 0.55: seg = apply_echo(seg, sr)\n        if random.random() < 0.60: seg = apply_reverb(seg, sr)\n        if random.random() < 0.30: seg = apply_clipping(seg)\n        if random.random() < 0.20: seg = bandpass_filter(seg, sr)\n        if random.random() < 0.25:\n            seg = librosa.effects.pitch_shift(seg, sr=sr, n_steps=random.uniform(-3, 3))\n        if random.random() < 0.25:\n            rate = random.uniform(0.80, 1.20)\n            seg  = librosa.effects.time_stretch(seg, rate=rate)\n            seg  = seg[:seg_samples] if len(seg) > seg_samples else np.pad(seg, (0, seg_samples - len(seg)))\n\n        # Normalize segment peak\n        mx = np.abs(seg).max()\n        if mx > 0:\n            seg = seg / mx * 0.9\n\n        augmented[start:end] = seg.astype(np.float32)\n        total_aug_time += seg_dur\n\n    return augmented.astype(np.float32)\n\n\nprint(\"âœ… Audio augmentation pipeline ready.\")","metadata":{"id":"R5CBL6ki0dxy"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA CLEANING: GARBAGE TEXT REMOVAL","metadata":{"id":"ppephILM0rMP"}},{"cell_type":"markdown","source":"- Strips non-Bengali characters from all transcript annotations before the dataset is built, improving label quality.","metadata":{"id":"k20kQPPm3SmT"}},{"cell_type":"code","source":"\nprint(\"ğŸ§¹ Cleaning annotation texts...\")\n\ndf_raw      = pd.read_csv(METADATA_PATH)\ndf_filtered = df_raw.iloc[DATA_START - 1 : DATA_END].copy()\n\n# Auto-detect the transcription column by common keyword patterns\ntranscription_col = next(\n    (c for c in df_filtered.columns\n     if any(k in c.lower() for k in (\"gt\", \"text\", \"annotation\", \"transcription\"))),\n    None,\n)\n\ntotal_cleaned, total_removed = 0, 0\n\nif transcription_col:\n    for idx, row in df_filtered.iterrows():\n        raw = row[transcription_col]\n        if pd.notna(raw):\n            cleaned, removed, had_garbage = clean_annotation_text(str(raw))\n            if had_garbage:\n                df_filtered.at[idx, transcription_col] = cleaned\n                total_cleaned += 1\n                total_removed += removed\n\n    print(f\"   Rows scanned   : {len(df_filtered)}\")\n    print(f\"   Rows cleaned   : {total_cleaned}\")\n    print(f\"   Chars removed  : {total_removed}\")\nelse:\n    print(\"âš ï¸  Transcription column not detected â€” skipping garbage removal.\")\n\ndf_metadata_cleaned = df_filtered.copy()\nprint(\"âœ… Annotation cleaning complete.\")","metadata":{"id":"hGTx_yN601mE"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATASET CONSTRUCTION","metadata":{"id":"UJlXkaj808K_"}},{"cell_type":"markdown","source":"- Validates audio files, normalizes transcripts, shuffles, and splits into train / validation sets.","metadata":{"id":"IHHGyb553QEb"}},{"cell_type":"code","source":"def load_dataset_from_csv(\n    metadata_path: str,\n    audio_base_path: str,\n    start_row: int = 1,\n    end_row: int   = 5000,\n    split_ratio: float = 0.9,\n    max_text_length: int = 1000,\n) -> DatasetDict:\n    \"\"\"Build a HuggingFace DatasetDict from a CSV metadata file.\n\n    Each row must contain an audio file path and a transcript.\n    Invalid paths, empty texts, and over-length texts are silently dropped.\n    \"\"\"\n    df = df_metadata_cleaned.copy() if \"df_metadata_cleaned\" in globals() \\\n         else pd.read_csv(metadata_path).iloc[start_row - 1 : end_row].copy()\n\n    cols = df.columns.tolist()\n    # Resolve column names for audio path and transcript\n    if   \"audio_path\" in cols and \"gt\"            in cols: path_col, text_col = \"audio_path\", \"gt\"\n    elif \"path\"       in cols and \"text\"          in cols: path_col, text_col = \"path\",       \"text\"\n    elif \"audio_path\" in cols and \"transcription\" in cols: path_col, text_col = \"audio_path\", \"transcription\"\n    else:                                                   path_col, text_col = cols[0],       cols[1]\n\n    valid, skipped = [], 0\n    for _, row in df.iterrows():\n        try:\n            # Resolve absolute audio path\n            ap = str(row[path_col]).strip()\n            if   ap.startswith(\"./\"): ap = os.path.join(audio_base_path, ap[2:])\n            elif not ap.startswith(\"/\"): ap = os.path.join(audio_base_path, ap)\n\n            text = normalize_text(row[text_col])\n\n            # Quality gates\n            if not text or len(text) < 3 or len(text) > max_text_length:\n                skipped += 1; continue\n            if not os.path.exists(ap) or os.path.getsize(ap) == 0:\n                skipped += 1; continue\n\n            valid.append({\"path\": ap, \"sentence\": text})\n        except Exception:\n            skipped += 1\n\n    if not valid:\n        raise ValueError(\"No valid samples found. Check METADATA_PATH and AUDIO_BASE_PATH.\")\n\n    random.shuffle(valid)\n    split      = max(1, int(len(valid) * split_ratio))\n    train_data = valid[:split]\n    val_data   = valid[split:]\n\n    def to_hf_dataset(data):\n        ds = Dataset.from_dict({\n            \"audio\":    [x[\"path\"]     for x in data],\n            \"sentence\": [x[\"sentence\"] for x in data],\n        })\n        return ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n    print(f\"   Valid samples  : {len(valid)}  (skipped {skipped})\")\n    print(f\"   Train / Val    : {len(train_data)} / {len(val_data)}\")\n    return DatasetDict({\"train\": to_hf_dataset(train_data), \"test\": to_hf_dataset(val_data)})\n\n\nprint(\"ğŸ“‚ Building dataset...\")\ndataset = load_dataset_from_csv(\n    METADATA_PATH, AUDIO_BASE_PATH,\n    start_row       = DATA_START,\n    end_row         = DATA_END,\n    split_ratio     = TRAIN_SPLIT,\n    max_text_length = MAX_TEXT_LENGTH,\n)\nprint(\"âœ… Dataset ready.\")","metadata":{"id":"xLAfaFUX0nBv"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FEATURE EXTRACTION & DATASET PREPROCESSING","metadata":{"id":"_PZkd4qi1Mz1"}},{"cell_type":"markdown","source":"- Converts raw audio arrays to log-mel spectrograms and tokenizes transcripts. Augmentation is applied only to train.","metadata":{"id":"z5iWsaxZ3J6-"}},{"cell_type":"code","source":"def prepare_dataset(batch: dict, augment: bool = False) -> dict:\n    \"\"\"Map function: audio waveform + transcript â†’ model input features.\"\"\"\n    audio = batch[\"audio\"]\n    arr   = audio[\"array\"].astype(np.float32)\n    sr    = audio[\"sampling_rate\"]\n\n    # Resample to 16 kHz if necessary\n    if sr != 16000:\n        arr = librosa.resample(arr, orig_sr=sr, target_sr=16000)\n        sr  = 16000\n\n    # Clip to max duration (Whisper supports up to 30 s)\n    max_samples = int(MAX_AUDIO_DURATION * sr)\n    if len(arr) > max_samples:\n        arr = arr[:max_samples]\n\n    # Stochastic augmentation (training only)\n    if augment and USE_AUGMENTATION and random.random() < AUGMENTATION_PROB:\n        arr = augment_audio_segments(arr, sr=sr)\n\n    batch[\"input_features\"] = feature_extractor(arr, sampling_rate=sr).input_features[0]\n    batch[\"labels\"]         = tokenizer(batch[\"sentence\"], max_length=448, truncation=True).input_ids\n    return batch\n\n\nprint(\"âš™ï¸  Preprocessing training set  (augmentation ON)...\")\ndataset[\"train\"] = dataset[\"train\"].map(\n    lambda x: prepare_dataset(x, augment=True),\n    remove_columns=[\"audio\", \"sentence\"],\n    num_proc=1,\n)\n\nprint(\"âš™ï¸  Preprocessing validation set (augmentation OFF)...\")\ndataset[\"test\"] = dataset[\"test\"].map(\n    lambda x: prepare_dataset(x, augment=False),\n    remove_columns=[\"audio\", \"sentence\"],\n    num_proc=1,\n)\n\nprint(\"âœ… Feature extraction complete.\")","metadata":{"id":"G83eie0b1Sn1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA COLLATOR & EVALUATION METRIC","metadata":{"id":"WyuQVrfF1f4h"}},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"Pad input features and label token sequences to the same length within a batch.\n\n    Labels are padded with -100 so that padding tokens are ignored by the loss.\n    The decoder start token is stripped from labels when it appears as the first token\n    (the model adds it automatically during forward pass).\n    \"\"\"\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        # Pad audio features\n        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Pad label sequences and mask padding with -100\n        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n        labels_batch   = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch.attention_mask.ne(1), -100\n        )\n        # Remove decoder start token if prepended (avoids double-counting)\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)\n\n# â”€â”€ WER Metric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwer_metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred) -> Dict[str, float]:\n    \"\"\"Decode predictions and references, then compute Word Error Rate.\"\"\"\n    pred_ids  = pred.predictions\n    label_ids = pred.label_ids\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n    pred_str  = tokenizer.batch_decode(pred_ids,  skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    return {\"wer\": 100 * wer_metric.compute(predictions=pred_str, references=label_str)}\n\n\nprint(\"âœ… Data collator and WER metric ready.\")","metadata":{"id":"tGrcZc611eEw"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HUGGINGFACE UPLOAD UTILITIES & CALLBACK","metadata":{"id":"99QGLdpI1ncg"}},{"cell_type":"markdown","source":"- Checkpoints are automatically pushed to the HF Hub after each save_steps interval.","metadata":{"id":"l8Lw2V_w2i9A"}},{"cell_type":"code","source":"def upload_to_hf(local_path: str, repo_id: str, commit_msg: str = None) -> None:\n    \"\"\"Upload a local folder to a HuggingFace dataset repository.\"\"\"\n    if not UPLOAD_CHECKPOINTS or not HF_TOKEN:\n        return\n    try:\n        name = os.path.basename(local_path)\n        HfApi().upload_folder(\n            folder_path    = local_path,\n            repo_id        = repo_id,\n            repo_type      = \"dataset\",\n            path_in_repo   = name,\n            commit_message = commit_msg or f\"Upload {name}\",\n            token          = HF_TOKEN,\n        )\n        print(f\"   ğŸ“¤ Uploaded â†’ {repo_id}/{name}\")\n    except Exception as e:\n        print(f\"   âš ï¸  Upload failed (non-fatal): {e}\")\n\n\nclass HFUploadCallback(TrainerCallback):\n    \"\"\"Trainer callback that pushes every saved checkpoint to HuggingFace Hub.\"\"\"\n\n    def __init__(self, repo_id: str):\n        self.repo_id = repo_id\n\n    def on_save(self, args, state, control, **kwargs):\n        if state.global_step > 0:\n            ckpt_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n            if os.path.exists(ckpt_path):\n                upload_to_hf(\n                    ckpt_path, self.repo_id,\n                    commit_msg=f\"Checkpoint â€” step {state.global_step}\",\n                )\n\n\nprint(\"âœ… Upload callback registered.\")","metadata":{"id":"4Pego9PW1mfb"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING ARGUMENTS- Optimizer","metadata":{"id":"gaGe8mi81xJt"}},{"cell_type":"markdown","source":"- Optimizer  : adamw_bnb_8bit (saves ~2.4 GB VRAM vs fp32 AdamW)\n\n\n\n- Precision  : fp16 mixed precision                  \n\n\n - Scheduler  : cosine with warmup","metadata":{"id":"DnIL-Y_Q17EI"}},{"cell_type":"code","source":"train_samples   = len(dataset[\"train\"])\nsteps_per_epoch = max(1, train_samples // (PER_DEVICE_TRAIN_BATCH * GRAD_ACCUM_STEPS))\ntotal_steps     = steps_per_epoch * TARGET_EPOCHS\neval_save_steps = 6000\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir = OUTPUT_PATH,\n\n    # â”€â”€ Batching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH,\n    gradient_accumulation_steps = GRAD_ACCUM_STEPS,\n    per_device_eval_batch_size  = PER_DEVICE_EVAL_BATCH,\n\n    # â”€â”€ Optimizer & Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    optim             = \"adamw_bnb_8bit\",   # 8-bit moments â†’ significant VRAM savings\n    learning_rate     = LEARNING_RATE,\n    warmup_steps      = WARMUP_STEPS,\n    lr_scheduler_type = \"cosine\",\n\n    # â”€â”€ Duration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    max_steps = total_steps,\n\n    # â”€â”€ Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    gradient_checkpointing = True,\n    fp16                   = True,\n    torch_compile          = False,\n\n    # â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    disable_tqdm       = False,\n    logging_first_step = True,\n    logging_steps      = 25,\n    log_level          = \"warning\",\n\n    # â”€â”€ Evaluation & Checkpointing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    evaluation_strategy = \"steps\",\n    eval_steps          = eval_save_steps,\n    save_steps          = eval_save_steps,\n    save_total_limit    = 2,           # Keep only the 2 most recent checkpoints\n\n    # â”€â”€ Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    predict_with_generate = True,\n    generation_max_length = 225,\n\n    # â”€â”€ Best-Model Tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    load_best_model_at_end = True,\n    metric_for_best_model  = \"wer\",\n    greater_is_better      = False,    # Lower WER is better\n\n    # â”€â”€ Miscellaneous â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    resume_from_checkpoint = RESUME_FROM_CHECKPOINT,\n    report_to              = [\"tensorboard\"],\n    remove_unused_columns  = False,\n    label_names            = [\"labels\"],\n    dataloader_num_workers = 0,\n)\n\nprint(\"âœ… Training arguments configured.\")\nprint(f\"   Train samples : {train_samples}  |  Steps/epoch : {steps_per_epoch}\")\nprint(f\"   Total steps   : {total_steps}  |  Eval/save every : {eval_save_steps} steps\")\nprint(f\"   Optimizer     : adamw_bnb_8bit  |  Precision : fp16\")","metadata":{"id":"XNvpJFlW16gC"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINER INITIALIZATION","metadata":{"id":"1b6JKNzt2Mos"}},{"cell_type":"code","source":"os.makedirs(OUTPUT_PATH, exist_ok=True)\nprocessor.save_pretrained(OUTPUT_PATH)   # Save processor alongside model artifacts\n\ncallbacks = [HFUploadCallback(repo_id=HF_OUTPUT_REPO)] if UPLOAD_CHECKPOINTS else []\n\ntrainer = Seq2SeqTrainer(\n    args            = training_args,\n    model           = model,\n    train_dataset   = dataset[\"train\"],\n    eval_dataset    = dataset[\"test\"],\n    data_collator   = data_collator,\n    compute_metrics = compute_metrics,\n    tokenizer       = processor.feature_extractor,\n    callbacks       = callbacks,\n)\n\nif device == \"cuda\":\n    used  = torch.cuda.memory_allocated() / 1024**3\n    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"ğŸ–¥ï¸  VRAM before training : {used:.2f} / {total:.2f} GB\")\n\nprint(\"âœ… Trainer ready.\")","metadata":{"id":"p1VGEmDl2LcZ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING EXECUTION","metadata":{"id":"fGimdqfh2Tra"}},{"cell_type":"code","source":"print(\"\\n\" + \"â•\" * 65)\nprint(\"ğŸš€  STARTING TRAINING\")\nprint(f\"    Model      : {MODEL_NAME}\")\nprint(f\"    Steps      : {total_steps}   ({TARGET_EPOCHS} epochs)\")\nprint(f\"    Optimizer  : adamw_bnb_8bit  |  GPU count : {torch.cuda.device_count()}\")\nprint(\"â•\" * 65 + \"\\n\")\n\ntry:\n    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n\n    # â”€â”€ Save Final Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(f\"\\nğŸ’¾ Saving final model â†’ {OUTPUT_PATH}\")\n    model.save_pretrained(OUTPUT_PATH)\n    processor.save_pretrained(OUTPUT_PATH)\n    print(\"âœ… Training complete. Final model saved.\")\n\n    # â”€â”€ Upload Final Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if UPLOAD_CHECKPOINTS and HF_TOKEN:\n        upload_to_hf(\n            OUTPUT_PATH, HF_OUTPUT_REPO,\n            commit_msg=f\"Final model â€” {TARGET_EPOCHS} epochs â€” {MODEL_NAME}\",\n        )\n        print(f\"âœ… Final model uploaded to HuggingFace : {HF_OUTPUT_REPO}\")\n\nexcept Exception as e:\n    import traceback\n    print(f\"\\nâŒ Training failed: {e}\")\n    traceback.print_exc()\n\nfinally:\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(\"\\nğŸ§¹ GPU memory cleared.\")","metadata":{"id":"Hq-xjaU72TFX"},"outputs":[],"execution_count":null}]}