{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":129271,"databundleVersionId":15506307,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ™ï¸ Speaker Diarization Fine-Tuning Notebook\n\nThis notebook demonstrates fine-tuning a speaker diarization model using PyAnnote.Audio.\n\n## Overview\n- **Model**: PyAnnote segmentation model 3.0\n- **Task**: Speaker diarization (who spoke when)\n- **Framework**: PyTorch Lightning\n- **Dataset**: DL SPRINT_4.0 Bengali Speaker Diarization\n\n## Workflow\n1. Install dependencies and setup environment\n2. Apply PyTorch patches for compatibility\n3. Load pretrained models\n4. Download and prepare dataset\n5. Create diarization task\n6. Evaluate pretrained model (baseline)\n7. Fine-tune model with checkpointing\n8. Evaluate fine-tuned model\n9. Run inference on test data\n\n---","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“¦ Section 1: Install Dependencies\n\nInstall PyAnnote.Audio and upgrade/downgrade specific packages for compatibility.","metadata":{}},{"cell_type":"code","source":"# Install PyAnnote.Audio version 4.0.0\n# This includes all necessary components for speaker diarization\n!pip install pyannote.audio==4.0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install torch 2.8.0 from the cu126 index\n!pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Install matching torchaudio and torchvision versions for torch 2.8.0\n!pip install torchaudio==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Downgrade torchcodec to 0.7\n!pip install torchcodec==0.7\n\nprint(\"âœ“ Dependencies installed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ”„ Section 2 â€” Restart Kernel\n\n> **âš ï¸ IMPORTANT:** After the cell above finishes, **restart the kernel** before continuing.\n> \n> Kaggle: **Run â†’ Restart & Clear Output** then run all cells from Section 3 onwards.","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“ Section 3: Data Preparation\n\nDownload annotation files from Hugging Face and prepare the dataset.\nSetup paths and load the database configuration.","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import list_repo_files, hf_hub_download\nimport shutil\n\n# =============================================================================\n# DATASET PATHS CONFIGURATION\n# =============================================================================\n\n# Base directory containing competition data\nBASE_DIR = r'/kaggle/input/competitions/dl-sprint-4-0-bengali-speaker-diarization-challenge'\n\n# Audio directories\nTRAIN_AUDIO_DIR = os.path.join(BASE_DIR, 'diarization', 'diarization', 'train', 'audio')\nTEST_AUDIO_DIR = os.path.join(BASE_DIR, 'diarization', 'diarization', 'test', 'audio')\n\n# Output directory for all training artifacts\nOUTPUT_DIR = '/kaggle/working/output'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"âœ“ Train audio directory: {TRAIN_AUDIO_DIR}\")\nprint(f\"âœ“ Test audio directory: {TEST_AUDIO_DIR}\")\nprint(f\"âœ“ Output directory: {OUTPUT_DIR}\")\n\n# =============================================================================\n# DOWNLOAD ANNOTATION FILES FROM HUGGING FACE\n# =============================================================================\n# Download RTTM, UEM, and LST files needed for training\n\nprint(\"\\nğŸ“¥ Downloading annotation files from Hugging Face...\")\n\nrepo_id = \"Sam3000/diarization_files\"\nrepo_type = \"dataset\"\ndownload_dir = \"./\"\n\n# List all files in the repository\nfiles = list_repo_files(repo_id=repo_id, repo_type=repo_type)\nprint(f\"ğŸ“ Found {len(files)} files in the repository\")\n\n# Download each file\nfor file in files:\n    print(f\"  Downloading: {file}\")\n    file_path = hf_hub_download(repo_id=repo_id, filename=file, repo_type=repo_type)\n    local_path = os.path.join(download_dir, file)\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    shutil.copy(file_path, local_path)\n\nprint(\"âœ“ All files downloaded\")\n\n# =============================================================================\n# EXTRACT ANNOTATION FILES\n# =============================================================================\n# Extract RTTM (reference time-marked), UEM (un-partitioned evaluation map),\n# and LST (list) files\n\nprint(\"\\nğŸ“¦ Extracting annotation files...\")\n\nos.system('unzip -q -o /kaggle/working/lst.zip -d /kaggle/working/')\nos.system('unzip -q -o /kaggle/working/rttm.zip -d /kaggle/working/')\nos.system('unzip -q -o /kaggle/working/uem.zip -d /kaggle/working/')\n\nprint(\"âœ“ Annotation files extracted\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyannote.database import get_protocol\n\n# =============================================================================\n# SET DATABASE CONFIGURATION\n# =============================================================================\n# Point PyAnnote to the database.yml configuration file\n# This file should contain the protocol definition and file paths\n\nos.environ['PYANNOTE_DATABASE_CONFIG'] = os.path.join('./', 'database.yml')\nprint(\"âœ“ Database configuration path set\")\n\n# =============================================================================\n# AUDIO PATH HELPER FUNCTION\n# =============================================================================\n\ndef get_audio_path(f):\n    \"\"\"\n    Convert file URI to full audio file path.\n    \n    Handles both training and test files:\n    - Files starting with 'train_' go to TRAIN_AUDIO_DIR\n    - Other files go to TEST_AUDIO_DIR\n    \n    Args:\n        f: File dictionary with 'uri' key\n        \n    Returns:\n        Full path to audio file\n    \"\"\"\n    uri = f['uri']\n    if uri.startswith('train_'):\n        return os.path.join(TRAIN_AUDIO_DIR, f'{uri}.wav')\n    return os.path.join(TEST_AUDIO_DIR, f'{uri}.wav')\n\n# =============================================================================\n# LOAD DATASET PROTOCOL\n# =============================================================================\n# Load the custom protocol defined in database.yml\n\nprint(\"\\nLoading dataset protocol...\")\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Count files in each split\ntrain_files = list(dataset.train())\ndev_files = list(dataset.development())\ntest_files = list(dataset.test())\n\nprint(f\"âœ“ Dataset loaded successfully\")\nprint(f\"  - Training files: {len(train_files)}\")\nprint(f\"  - Development files: {len(dev_files)}\")\nprint(f\"  - Test files: {len(test_files)}\")\nprint(f\"  - Total files: {len(train_files) + len(dev_files) + len(test_files)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport numpy as np\n\n# =============================================================================\n# PYTORCH PATCHES FOR CHECKPOINT LOADING\n# =============================================================================\n# Patch torch.load to allow loading pyannote checkpoints\n# PyAnnote models may contain unpickled objects that require weights_only=False\n\n_orig_load = torch.load\n\ndef _patched_load(f, *args, **kwargs):\n    \"\"\"Patched torch.load that disables weights_only restriction.\"\"\"\n    kwargs['weights_only'] = False\n    return _orig_load(f, *args, **kwargs)\n\ntorch.load = _patched_load\nprint(\"âœ“ PyTorch load function patched for checkpoint compatibility\")\n\n# =============================================================================\n# NUMPY COMPATIBILITY PATCHES\n# =============================================================================\n# Add np.NaN and np.Inf if they don't exist (for older numpy versions)\n\nif not hasattr(np, 'NaN'):\n    np.NaN = np.nan\n    print(\"âœ“ Added np.NaN attribute\")\n\nif not hasattr(np, 'Inf'):\n    np.Inf = np.inf\n    print(\"âœ“ Added np.Inf attribute\")\n\n# =============================================================================\n# GPU VERIFICATION\n# =============================================================================\n# Verify GPU availability (required for training)\n\nassert torch.cuda.is_available(), 'No GPU available! Please enable GPU in Kaggle settings.'\n\ndevice = torch.device('cuda')\nprint(f'\\nâœ“ PyTorch {torch.__version__}')\nprint(f'âœ“ GPU: {torch.cuda.get_device_name(0)}')\nprint(f'âœ“ Device: {device}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ¤– Section 4: Load Pretrained Model & Pipeline\n\nLoad the pretrained segmentation model and full diarization pipeline from Hugging Face.","metadata":{}},{"cell_type":"code","source":"from pyannote.audio import Model, Inference, Pipeline\n\n# =============================================================================\n# HUGGING FACE AUTHENTICATION\n# =============================================================================\n# Required for downloading pretrained models from Hugging Face Hub\n# Get your token from: https://huggingface.co/settings/tokens\n\nHF_TOKEN = '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'\n\n# =============================================================================\n# LOAD PRETRAINED SEGMENTATION MODEL\n# =============================================================================\n# This model will be fine-tuned on our custom dataset\n# Using segmentation-3.0 for speaker activity detection\n\nprint(\"Loading pretrained segmentation model...\")\npretrained = Model.from_pretrained(\n    'pyannote/segmentation-3.0',\n    token=HF_TOKEN\n)\nprint(\"âœ“ Pretrained segmentation model loaded\")\n\n# =============================================================================\n# LOAD FULL DIARIZATION PIPELINE\n# =============================================================================\n# This pipeline will be used for evaluation\n# It includes segmentation, embedding, and clustering components\n\nprint(\"\\nLoading pretrained diarization pipeline...\")\npipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-community-1\",\n    token=HF_TOKEN\n)\npipeline.to(device)\nprint(\"âœ“ Pretrained pipeline loaded and moved to GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## ğŸ¯ Section 5: Create Speaker Diarization Task\n\nConfigure the speaker diarization task with training parameters.","metadata":{}},{"cell_type":"code","source":"from pyannote.audio.tasks import SpeakerDiarization as SpeakerDiarizationTask\n\n# =============================================================================\n# RELOAD PROTOCOL\n# =============================================================================\n# Protocol iterators can only be consumed once, so reload for task creation\n\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# =============================================================================\n# CREATE SPEAKER DIARIZATION TASK\n# =============================================================================\n\nprint(\"Creating speaker diarization task...\")\n\nseg_task = SpeakerDiarizationTask(\n    dataset,\n    duration=10.0,                    # Length of audio chunks (seconds)\n    max_speakers_per_chunk=3,         # Maximum speakers per chunk\n    max_speakers_per_frame=1          # Maximum simultaneous speakers per frame\n)\n\nprint(\"âœ“ SpeakerDiarization task created\")\nprint(f\"  - Chunk duration: 10.0 seconds\")\nprint(f\"  - Max speakers per chunk: 3\")\nprint(f\"  - Max speakers per frame: 1\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ“Š Section 6: Evaluate Pretrained Pipeline (Baseline)\n\nEstablish baseline performance by evaluating the pretrained model on our dataset.","metadata":{}},{"cell_type":"code","source":"from pyannote.metrics.diarization import DiarizationErrorRate\nfrom pyannote.core import Annotation, Segment\n\n# =============================================================================\n# HELPER FUNCTION: CONVERT TO ANNOTATION\n# =============================================================================\n\ndef to_annotation(pipeline_output):\n    \"\"\"\n    Convert pipeline output to Annotation object.\n    \n    Handles multiple PyAnnote versions:\n    - PyAnnote 3.x: Returns Annotation directly\n    - PyAnnote 4.x: Returns DiarizeOutput with .speaker_diarization attribute\n    - Fallback: Uses itertracks() method\n    \n    Args:\n        pipeline_output: Output from diarization pipeline\n        \n    Returns:\n        Annotation object with speaker segments\n        \n    Raises:\n        TypeError: If output format is not recognized\n    \"\"\"\n    # PyAnnote 3.x: Already an Annotation\n    if isinstance(pipeline_output, Annotation):\n        return pipeline_output\n    \n    # PyAnnote 4.x: DiarizeOutput with .speaker_diarization\n    if hasattr(pipeline_output, 'speaker_diarization'):\n        ann = Annotation()\n        for turn, speaker in pipeline_output.speaker_diarization:\n            ann[Segment(turn.start, turn.end)] = speaker\n        return ann\n    \n    # Fallback: Try itertracks method\n    if hasattr(pipeline_output, 'itertracks'):\n        ann = Annotation()\n        for turn, _, speaker in pipeline_output.itertracks(yield_label=True):\n            ann[Segment(turn.start, turn.end)] = speaker\n        return ann\n    \n    raise TypeError(f\"Cannot convert {type(pipeline_output)} to Annotation\")\n\n# =============================================================================\n# EVALUATE PRETRAINED MODEL\n# =============================================================================\n\nprint(\"Evaluating pretrained pipeline on development set...\")\nprint(\"This may take several minutes...\\n\")\n\n# Initialize Diarization Error Rate metric\nmetric = DiarizationErrorRate()\n\n# Reload protocol for evaluation\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Evaluate on development set\ndev_files_list = list(dataset.development())\nfor i, file in enumerate(dev_files_list, 1):\n    print(f\"  [{i}/{len(dev_files_list)}] Processing: {file['uri']}\")\n    \n    # Run diarization\n    hyp = to_annotation(pipeline(file))\n    \n    # Store hypothesis\n    file['pretrained pipeline'] = hyp\n    \n    # Update metric with ground truth and hypothesis\n    metric(file['annotation'], hyp, uem=file['annotated'])\n\n# Calculate final DER\nder_pretrained = abs(metric)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASELINE PERFORMANCE (Pretrained Model)\")\nprint(\"=\"*60)\nprint(f\"Diarization Error Rate (DER): {100 * der_pretrained:.1f}%\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ‹ï¸ Section 7: Fine-Tune the Model (with Checkpointing)\n\nFine-tune the segmentation model on our custom dataset.\nCheckpoints will be saved to the output directory.","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# =============================================================================\n# CLONE MODEL FOR FINE-TUNING\n# =============================================================================\n# Create a deep copy to preserve the original pretrained model\n\nprint(\"Creating model for fine-tuning...\")\nfinetuned = deepcopy(pretrained)\nfinetuned.task = seg_task\nprint(\"âœ“ Model cloned and task assigned\")\n\n# =============================================================================\n# SETUP CHECKPOINT CALLBACK\n# =============================================================================\n# Save checkpoints during training to the output directory\n\ncheckpoint_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nprint(f\"\\nâœ“ Checkpoint directory: {checkpoint_dir}\")\n\n# ModelCheckpoint callback configuration\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=checkpoint_dir,\n    filename='best-model-{epoch:02d}-{loss:.2f}',\n    save_top_k=1,              # Save only the best checkpoint\n    monitor='loss/train',      # Monitor training loss\n    mode='min',                # Save checkpoint with minimum loss\n    save_last=True,            # Also save the last checkpoint\n    verbose=True\n)\n\nprint(\"âœ“ Checkpoint callback configured\")\nprint(f\"  - Monitoring: loss/train\")\nprint(f\"  - Saving best model based on minimum loss\")\nprint(f\"  - Also saving last checkpoint\")\n\n# =============================================================================\n# SETUP TRAINER\n# =============================================================================\n\nprint(\"\\nConfiguring PyTorch Lightning trainer...\")\n\ntrainer = pl.Trainer(\n    devices=1,\n    accelerator=\"gpu\",\n    max_epochs=50,\n    default_root_dir=OUTPUT_DIR,\n    callbacks=[checkpoint_callback],\n    enable_checkpointing=True,\n)\n\nprint(\"âœ“ Trainer configured\")\nprint(f\"  - Max epochs: 50\")\nprint(f\"  - Accelerator: GPU\")\nprint(f\"  - Checkpointing: Enabled\")\n\n# =============================================================================\n# START TRAINING\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\"*60 + \"\\n\")\n\n# Run training\ntrainer.fit(finetuned)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINE-TUNING COMPLETE\")\nprint(\"=\"*60)\n\n# =============================================================================\n# VERIFY CHECKPOINTS SAVED\n# =============================================================================\n\nif os.path.exists(checkpoint_dir):\n    checkpoints = os.listdir(checkpoint_dir)\n    print(f\"\\nâœ“ Checkpoints saved:\")\n    for ckpt in checkpoints:\n        ckpt_path = os.path.join(checkpoint_dir, ckpt)\n        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n        print(f\"  - {ckpt} ({size_mb:.1f} MB)\")\n    \n    print(f\"\\nâœ“ Best checkpoint path: {checkpoint_callback.best_model_path}\")\nelse:\n    print(\"\\nâš ï¸  WARNING: No checkpoints directory found!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ“ˆ Section 8: Evaluate Fine-Tuned Pipeline\n\nLoad the best checkpoint and evaluate the fine-tuned model's performance.\nCompare with baseline to measure improvement.","metadata":{}},{"cell_type":"code","source":"import glob\n\n# =============================================================================\n# LOAD BEST CHECKPOINT\n# =============================================================================\n\nprint(\"Loading best checkpoint...\")\n\n# Search for all checkpoint files recursively\nckpt_pattern = os.path.join(OUTPUT_DIR, '**', '*.ckpt')\nckpts = glob.glob(ckpt_pattern, recursive=True)\n\nif ckpts:\n    # Prefer \"best\" checkpoint, otherwise use last one\n    best_ckpts = [c for c in ckpts if 'best' in os.path.basename(c).lower()]\n    ckpt_path = best_ckpts[0] if best_ckpts else sorted(ckpts)[-1]\n    \n    print(f\"âœ“ Loading checkpoint: {os.path.basename(ckpt_path)}\")\n    print(f\"  Path: {ckpt_path}\")\n    \n    # Load checkpoint and extract model state\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    finetuned.load_state_dict(checkpoint['state_dict'])\n    \n    print(\"âœ“ Checkpoint loaded successfully\")\nelse:\n    print(\"âš ï¸  No checkpoint found, using model from end of training\")\n\n# Set model to evaluation mode\nfinetuned.eval()\nfinetuned.to(device)\n\n# =============================================================================\n# BUILD FINE-TUNED PIPELINE\n# =============================================================================\n# Replace the segmentation component with our fine-tuned model\n\nprint(\"\\nBuilding fine-tuned pipeline...\")\n\n# Load fresh pipeline (uses pretrained components)\nfinetuned_pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-community-1\",\n    token=HF_TOKEN\n)\n\n# Replace the segmentation Inference object with our fine-tuned model\nfinetuned_pipeline._segmentation = Inference(\n    finetuned,\n    duration=finetuned_pipeline._segmentation.duration,\n    step=finetuned_pipeline._segmentation.step,\n    skip_aggregation=finetuned_pipeline._segmentation.skip_aggregation,\n    batch_size=finetuned_pipeline._segmentation.batch_size,\n)\nfinetuned_pipeline.to(device)\n\nprint(\"âœ“ Fine-tuned pipeline ready (segmentation model swapped)\")\n\n# =============================================================================\n# VERIFY MODEL SWAP\n# =============================================================================\n\nassert finetuned_pipeline._segmentation.model is finetuned, \\\n    \"ERROR: Model swap failed! Pipeline is still using the pretrained model.\"\n\nprint(\"âœ“ Verified: Pipeline is using fine-tuned segmentation model\")\n\n# =============================================================================\n# EVALUATE ON DEVELOPMENT SET\n# =============================================================================\n\nprint(\"\\nEvaluating fine-tuned model on development set...\")\nprint(\"This may take several minutes...\\n\")\n\n# Initialize metric\nmetric = DiarizationErrorRate()\n\n# Reload dataset\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Evaluate each file\ndev_files_list = list(dataset.development())\nfor i, file in enumerate(dev_files_list, 1):\n    print(f\"  [{i}/{len(dev_files_list)}] Processing: {file['uri']}\")\n    \n    # Run diarization\n    hyp = to_annotation(finetuned_pipeline(file))\n    \n    # Store hypothesis\n    file['finetuned pipeline'] = hyp\n    \n    # Update metric\n    metric(file['annotation'], hyp, uem=file['annotated'])\n\n# Calculate final DER\nder_finetuned = abs(metric)\n\n# =============================================================================\n# COMPARE RESULTS\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS - COMPARISON\")\nprint(\"=\"*60)\nprint(f\"\\nPretrained Model DER:  {100 * der_pretrained:.1f}%\")\nprint(f\"Fine-tuned Model DER:  {100 * der_finetuned:.1f}%\")\n\n# Calculate improvement metrics\nimprovement = (der_pretrained - der_finetuned) / der_pretrained * 100\nabsolute_improvement = 100 * (der_pretrained - der_finetuned)\n\nif improvement > 0:\n    print(f\"\\nâœ“ Relative Improvement: {improvement:.1f}%\")\n    print(f\"âœ“ Absolute Improvement: {absolute_improvement:.1f} percentage points\")\n    print(f\"\\nğŸ‰ Fine-tuning was successful!\")\nelse:\n    print(f\"\\nâš ï¸  No improvement detected\")\n    print(f\"   DER increased by: {-absolute_improvement:.1f} percentage points\")\n    print(\"\\n   Suggestions:\")\n    print(\"   - Train for more epochs\")\n    print(\"   - Adjust learning rate\")\n    print(\"   - Check data quality\")\n    print(\"   - Verify annotation accuracy\")\n\nprint(\"=\"*60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}