{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":129271,"databundleVersionId":15506307,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ™ï¸ Speaker Diarization Fine-Tuning Notebook\n\nThis notebook demonstrates fine-tuning a speaker diarization model using PyAnnote.Audio.\n\n## Overview\n- **Model**: PyAnnote segmentation model 3.0\n- **Task**: Speaker diarization (who spoke when)\n- **Framework**: PyTorch Lightning\n- **Dataset**: DL SPRINT_4.0 Bengali Speaker Diarization\n\n## Workflow\n1. Install dependencies and setup environment\n2. Apply PyTorch patches for compatibility\n3. Load pretrained models\n4. Download and prepare dataset\n5. Create diarization task\n6. Evaluate pretrained model (baseline)\n7. Fine-tune model with checkpointing\n8. Evaluate fine-tuned model\n9. Run inference on test data\n\n---","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“¦ Section 1: Install Dependencies\n\nInstall PyAnnote.Audio and upgrade/downgrade specific packages for compatibility.","metadata":{}},{"cell_type":"code","source":"# Install PyAnnote.Audio version 4.0.0\n# This includes all necessary components for speaker diarization\n!pip install pyannote.audio==4.0.0","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2026-02-22T11:17:38.329596Z","iopub.status.busy":"2026-02-22T11:17:38.329389Z","iopub.status.idle":"2026-02-22T11:17:56.431801Z","shell.execute_reply":"2026-02-22T11:17:56.430935Z","shell.execute_reply.started":"2026-02-22T11:17:38.329574Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyannote.audio==4.0.0\n","  Downloading pyannote_audio-4.0.0-py3-none-any.whl.metadata (14 kB)\n","Collecting asteroid-filterbanks>=0.4.0 (from pyannote.audio==4.0.0)\n","  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (0.8.1)\n","Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (1.4.1)\n","Collecting lightning>=2.4 (from pyannote.audio==4.0.0)\n","  Downloading lightning-2.6.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (3.10.0)\n","Collecting opentelemetry-api==1.34.0 (from pyannote.audio==4.0.0)\n","  Downloading opentelemetry_api-1.34.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-exporter-otlp==1.34.0 (from pyannote.audio==4.0.0)\n","  Downloading opentelemetry_exporter_otlp-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk==1.34.0 (from pyannote.audio==4.0.0)\n","  Downloading opentelemetry_sdk-1.34.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting pyannote-core>=6.0.1 (from pyannote.audio==4.0.0)\n","  Downloading pyannote_core-6.0.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting pyannote-database>=6.0.0 (from pyannote.audio==4.0.0)\n","  Downloading pyannote_database-6.1.1-py3-none-any.whl.metadata (30 kB)\n","Collecting pyannote-metrics>=4.0.0 (from pyannote.audio==4.0.0)\n","  Downloading pyannote_metrics-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting pyannote-pipeline>=4.0.0 (from pyannote.audio==4.0.0)\n","  Downloading pyannote_pipeline-4.0.0-py3-none-any.whl.metadata (5.4 kB)\n","Collecting pyannoteai-sdk>=0.2.1 (from pyannote.audio==4.0.0)\n","  Downloading pyannoteai_sdk-0.4.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting pytorch-metric-learning>=2.8.1 (from pyannote.audio==4.0.0)\n","  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (13.9.4)\n","Requirement already satisfied: safetensors>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (0.7.0)\n","Requirement already satisfied: soundfile>=0.13.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (0.13.1)\n","Collecting torch-audiomentations>=0.12.0 (from pyannote.audio==4.0.0)\n","  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (2.9.0+cu126)\n","Requirement already satisfied: torchaudio>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (2.9.0+cu126)\n","Requirement already satisfied: torchcodec>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (0.9.0)\n","Requirement already satisfied: torchmetrics>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.audio==4.0.0) (1.8.2)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.34.0->pyannote.audio==4.0.0) (8.7.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api==1.34.0->pyannote.audio==4.0.0) (4.15.0)\n","Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.0 (from opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-exporter-otlp-proto-http==1.34.0 (from opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0)\n","  Downloading opentelemetry_exporter_otlp_proto_http-1.34.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-semantic-conventions==0.55b0 (from opentelemetry-sdk==1.34.0->pyannote.audio==4.0.0)\n","  Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (1.72.0)\n","Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (1.76.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting opentelemetry-proto==1.34.0 (from opentelemetry-exporter-otlp-proto-grpc==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0)\n","  Downloading opentelemetry_proto-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: requests~=2.7 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (2.32.4)\n","Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.34.0->opentelemetry-exporter-otlp-proto-grpc==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (5.29.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from asteroid-filterbanks>=0.4.0->pyannote.audio==4.0.0) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (1.2.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (0.28.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (6.0.3)\n","Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (1.5.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (4.67.1)\n","Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (0.21.1)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.4->pyannote.audio==4.0.0) (0.15.2)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning>=2.4->pyannote.audio==4.0.0) (2.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.0->pyannote.audio==4.0.0) (2.9.0.post0)\n","Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from pyannote-core>=6.0.1->pyannote.audio==4.0.0) (2.3.3)\n","Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-core>=6.0.1->pyannote.audio==4.0.0) (2.4.0)\n","Collecting numpy (from asteroid-filterbanks>=0.4.0->pyannote.audio==4.0.0)\n","  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-metrics>=4.0.0->pyannote.audio==4.0.0) (1.6.1)\n","Requirement already satisfied: scipy>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-metrics>=4.0.0->pyannote.audio==4.0.0) (1.16.3)\n","Requirement already satisfied: optuna>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (4.7.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->pyannote.audio==4.0.0) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->pyannote.audio==4.0.0) (2.19.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.13.1->pyannote.audio==4.0.0) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->pyannote.audio==4.0.0) (3.5.0)\n","Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.12.0->pyannote.audio==4.0.0)\n","  Downloading julius-0.2.7.tar.gz (59 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.12.0->pyannote.audio==4.0.0)\n","  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.13.1->pyannote.audio==4.0.0) (2.23)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (3.13.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (4.12.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (2026.1.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (0.16.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.34.0->pyannote.audio==4.0.0) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->pyannote.audio==4.0.0) (0.1.2)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=4.2.0->pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (1.18.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna>=4.2.0->pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (6.10.1)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=4.2.0->pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (2.0.45)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->pyannote-core>=6.0.1->pyannote.audio==4.0.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->pyannote-core>=6.0.1->pyannote.audio==4.0.0) (2025.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.10.0->pyannote.audio==4.0.0) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.34.0->opentelemetry-exporter-otlp==1.34.0->pyannote.audio==4.0.0) (2.5.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->pyannote-metrics>=4.0.0->pyannote.audio==4.0.0) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->pyannote-metrics>=4.0.0->pyannote.audio==4.0.0) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.8.0->pyannote.audio==4.0.0) (1.3.0)\n","Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.12.0->pyannote.audio==4.0.0)\n","  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.8.0->pyannote.audio==4.0.0) (3.0.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.28.1->pyannote.audio==4.0.0) (8.3.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning>=2.4->pyannote.audio==4.0.0) (1.22.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=4.2.0->pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (1.3.10)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=4.2.0->pyannote-pipeline>=4.0.0->pyannote.audio==4.0.0) (3.3.0)\n","Downloading pyannote_audio-4.0.0-py3-none-any.whl (891 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.34.0-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.0-py3-none-any.whl (7.0 kB)\n","Downloading opentelemetry_sdk-1.34.0-py3-none-any.whl (118 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.4/118.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_http-1.34.0-py3-none-any.whl (17 kB)\n","Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl (196 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.34.0-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n","Downloading lightning-2.6.1-py3-none-any.whl (853 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m853.6/853.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote_core-6.0.1-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote_database-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote_metrics-4.0.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyannote_pipeline-4.0.0-py3-none-any.whl (22 kB)\n","Downloading pyannoteai_sdk-0.4.0-py3-none-any.whl (8.9 kB)\n","Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n","Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n","Building wheels for collected packages: julius\n","  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=98b4cb4f342fd29a355befc4e2207072829eef84e1d8f6b081fb5d0d780f051e\n","  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n","Successfully built julius\n","Installing collected packages: primePy, opentelemetry-proto, numpy, pyannoteai-sdk, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, pyannote-core, opentelemetry-semantic-conventions, pytorch-metric-learning, pyannote-database, opentelemetry-sdk, julius, asteroid-filterbanks, torch-pitch-shift, pyannote-pipeline, pyannote-metrics, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, torch-audiomentations, opentelemetry-exporter-otlp, lightning, pyannote.audio\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.37.0\n","    Uninstalling opentelemetry-proto-1.37.0:\n","      Successfully uninstalled opentelemetry-proto-1.37.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.37.0\n","    Uninstalling opentelemetry-api-1.37.0:\n","      Successfully uninstalled opentelemetry-api-1.37.0\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n","    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.37.0\n","    Uninstalling opentelemetry-sdk-1.37.0:\n","      Successfully uninstalled opentelemetry-sdk-1.37.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-http\n","    Found existing installation: opentelemetry-exporter-otlp-proto-http 1.37.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-http-1.37.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-http-1.37.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n","google-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n","ydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\n","google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n","dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.34.0 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.34.0 which is incompatible.\n","google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.34.0 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n","opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.34.0 which is incompatible.\n","opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.34.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asteroid-filterbanks-0.4.0 julius-0.2.7 lightning-2.6.1 numpy-2.4.2 opentelemetry-api-1.34.0 opentelemetry-exporter-otlp-1.34.0 opentelemetry-exporter-otlp-proto-common-1.34.0 opentelemetry-exporter-otlp-proto-grpc-1.34.0 opentelemetry-exporter-otlp-proto-http-1.34.0 opentelemetry-proto-1.34.0 opentelemetry-sdk-1.34.0 opentelemetry-semantic-conventions-0.55b0 primePy-1.3 pyannote-core-6.0.1 pyannote-database-6.1.1 pyannote-metrics-4.0.0 pyannote-pipeline-4.0.0 pyannote.audio-4.0.0 pyannoteai-sdk-0.4.0 pytorch-metric-learning-2.9.0 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5\n"]}],"execution_count":1},{"cell_type":"code","source":"# Install torch 2.8.0 from the cu126 index\n!pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Install matching torchaudio and torchvision versions for torch 2.8.0\n!pip install torchaudio==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu126\n\n# Downgrade torchcodec to 0.7\n!pip install torchcodec==0.7\n\nprint(\"âœ“ Dependencies installed\")","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2026-02-22T11:17:56.434265Z","iopub.status.busy":"2026-02-22T11:17:56.433890Z","iopub.status.idle":"2026-02-22T11:19:25.944721Z","shell.execute_reply":"2026-02-22T11:19:25.944061Z","shell.execute_reply.started":"2026-02-22T11:17:56.434233Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://download.pytorch.org/whl/cu126\n","Collecting torch==2.8.0\n","  Downloading https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n","  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.11.1.6)\n","Collecting triton==3.4.0 (from torch==2.8.0)\n","  Downloading https://download.pytorch.org/whl/triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n","Downloading https://download.pytorch.org/whl/cu126/torch-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (821.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.8/821.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.5.0\n","    Uninstalling triton-3.5.0:\n","      Successfully uninstalled triton-3.5.0\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.5\n","    Uninstalling nvidia-nccl-cu12-2.27.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.9.0+cu126\n","    Uninstalling torch-2.9.0+cu126:\n","      Successfully uninstalled torch-2.9.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.8.0+cu126 which is incompatible.\n","torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.8.0+cu126 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.27.3 torch-2.8.0+cu126 triton-3.4.0\n","Looking in indexes: https://download.pytorch.org/whl/cu126\n","Collecting torchaudio==2.8.0\n","  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n","Collecting torchvision==0.23.0\n","  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio==2.8.0) (2.8.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.4.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.20.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio==2.8.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio==2.8.0) (3.0.3)\n","Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision, torchaudio\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.24.0+cu126\n","    Uninstalling torchvision-0.24.0+cu126:\n","      Successfully uninstalled torchvision-0.24.0+cu126\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.9.0+cu126\n","    Uninstalling torchaudio-2.9.0+cu126:\n","      Successfully uninstalled torchaudio-2.9.0+cu126\n","Successfully installed torchaudio-2.8.0+cu126 torchvision-0.23.0+cu126\n","Collecting torchcodec==0.7\n","  Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n","Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torchcodec\n","  Attempting uninstall: torchcodec\n","    Found existing installation: torchcodec 0.9.0\n","    Uninstalling torchcodec-0.9.0:\n","      Successfully uninstalled torchcodec-0.9.0\n","Successfully installed torchcodec-0.7.0\n","âœ“ Dependencies installed\n"]}],"execution_count":2},{"cell_type":"markdown","source":"## ğŸ”„ Section 2 â€” Restart Kernel\n\n> **âš ï¸ IMPORTANT:** After the cell above finishes, **restart the kernel** before continuing.\n> \n> Kaggle: **Run â†’ Restart & Clear Output** then run all cells from Section 3 onwards.","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“ Section 3: Data Preparation\n\nDownload annotation files from Hugging Face and prepare the dataset.\nSetup paths and load the database configuration.","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import list_repo_files, hf_hub_download\nimport shutil\n\n# =============================================================================\n# DATASET PATHS CONFIGURATION\n# =============================================================================\n\n# Base directory containing competition data\nBASE_DIR = r'/kaggle/input/competitions/dl-sprint-4-0-bengali-speaker-diarization-challenge'\n\n# Audio directories\nTRAIN_AUDIO_DIR = os.path.join(BASE_DIR, 'diarization', 'diarization', 'train', 'audio')\nTEST_AUDIO_DIR = os.path.join(BASE_DIR, 'diarization', 'diarization', 'test', 'audio')\n\n# Output directory for all training artifacts\nOUTPUT_DIR = '/kaggle/working/output'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"âœ“ Train audio directory: {TRAIN_AUDIO_DIR}\")\nprint(f\"âœ“ Test audio directory: {TEST_AUDIO_DIR}\")\nprint(f\"âœ“ Output directory: {OUTPUT_DIR}\")\n\n# =============================================================================\n# DOWNLOAD ANNOTATION FILES FROM HUGGING FACE\n# =============================================================================\n# Download RTTM, UEM, and LST files needed for training\n\nprint(\"\\nğŸ“¥ Downloading annotation files from Hugging Face...\")\n\nrepo_id = \"Sam3000/diarization_files\"\nrepo_type = \"dataset\"\ndownload_dir = \"./\"\n\n# List all files in the repository\nfiles = list_repo_files(repo_id=repo_id, repo_type=repo_type)\nprint(f\"ğŸ“ Found {len(files)} files in the repository\")\n\n# Download each file\nfor file in files:\n    print(f\"  Downloading: {file}\")\n    file_path = hf_hub_download(repo_id=repo_id, filename=file, repo_type=repo_type)\n    local_path = os.path.join(download_dir, file)\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    shutil.copy(file_path, local_path)\n\nprint(\"âœ“ All files downloaded\")\n\n# =============================================================================\n# EXTRACT ANNOTATION FILES\n# =============================================================================\n# Extract RTTM (reference time-marked), UEM (un-partitioned evaluation map),\n# and LST (list) files\n\nprint(\"\\nğŸ“¦ Extracting annotation files...\")\n\nos.system('unzip -q -o /kaggle/working/lst.zip -d /kaggle/working/')\nos.system('unzip -q -o /kaggle/working/rttm.zip -d /kaggle/working/')\nos.system('unzip -q -o /kaggle/working/uem.zip -d /kaggle/working/')\n\nprint(\"âœ“ Annotation files extracted\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:20:32.675849Z","iopub.status.busy":"2026-02-22T11:20:32.675091Z","iopub.status.idle":"2026-02-22T11:20:35.900574Z","shell.execute_reply":"2026-02-22T11:20:35.900030Z","shell.execute_reply.started":"2026-02-22T11:20:32.675820Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ“ Train audio directory: /kaggle/input/competitions/dl-sprint-4-0-bengali-speaker-diarization-challenge/diarization/diarization/train/audio\n","âœ“ Test audio directory: /kaggle/input/competitions/dl-sprint-4-0-bengali-speaker-diarization-challenge/diarization/diarization/test/audio\n","âœ“ Output directory: /kaggle/working/output\n","\n","ğŸ“¥ Downloading annotation files from Hugging Face...\n","ğŸ“ Found 6 files in the repository\n","  Downloading: .gitattributes\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29a43361c0f5433aabf06e4fb830d443","version_major":2,"version_minor":0},"text/plain":[".gitattributes: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Downloading: database.yml\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a73cdb47fcd44cf8f0f7fec60d6c230","version_major":2,"version_minor":0},"text/plain":["database.yml:   0%|          | 0.00/535 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Downloading: lst.zip\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9900c643a0f4a539b752a43a06f2018","version_major":2,"version_minor":0},"text/plain":["lst.zip:   0%|          | 0.00/799 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Downloading: requirements.txt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2c5de8394d84b90a80e975f918b6530","version_major":2,"version_minor":0},"text/plain":["requirements.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Downloading: rttm.zip\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a59e10658ee42c986373ad7e038c502","version_major":2,"version_minor":0},"text/plain":["rttm.zip:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Downloading: uem.zip\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39d18e29d0ce41fea2d12ff08c14945d","version_major":2,"version_minor":0},"text/plain":["uem.zip:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ“ All files downloaded\n","\n","ğŸ“¦ Extracting annotation files...\n","âœ“ Annotation files extracted\n"]}],"execution_count":1},{"cell_type":"code","source":"from pyannote.database import get_protocol\n\n# =============================================================================\n# SET DATABASE CONFIGURATION\n# =============================================================================\n# Point PyAnnote to the database.yml configuration file\n# This file should contain the protocol definition and file paths\n\nos.environ['PYANNOTE_DATABASE_CONFIG'] = os.path.join('./', 'database.yml')\nprint(\"âœ“ Database configuration path set\")\n\n# =============================================================================\n# AUDIO PATH HELPER FUNCTION\n# =============================================================================\n\ndef get_audio_path(f):\n    \"\"\"\n    Convert file URI to full audio file path.\n    \n    Handles both training and test files:\n    - Files starting with 'train_' go to TRAIN_AUDIO_DIR\n    - Other files go to TEST_AUDIO_DIR\n    \n    Args:\n        f: File dictionary with 'uri' key\n        \n    Returns:\n        Full path to audio file\n    \"\"\"\n    uri = f['uri']\n    if uri.startswith('train_'):\n        return os.path.join(TRAIN_AUDIO_DIR, f'{uri}.wav')\n    return os.path.join(TEST_AUDIO_DIR, f'{uri}.wav')\n\n# =============================================================================\n# LOAD DATASET PROTOCOL\n# =============================================================================\n# Load the custom protocol defined in database.yml\n\nprint(\"\\nLoading dataset protocol...\")\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Count files in each split\ntrain_files = list(dataset.train())\ndev_files = list(dataset.development())\ntest_files = list(dataset.test())\n\nprint(f\"âœ“ Dataset loaded successfully\")\nprint(f\"  - Training files: {len(train_files)}\")\nprint(f\"  - Development files: {len(dev_files)}\")\nprint(f\"  - Test files: {len(test_files)}\")\nprint(f\"  - Total files: {len(train_files) + len(dev_files) + len(test_files)}\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:20:37.891416Z","iopub.status.busy":"2026-02-22T11:20:37.891116Z","iopub.status.idle":"2026-02-22T11:20:38.569893Z","shell.execute_reply":"2026-02-22T11:20:38.569284Z","shell.execute_reply.started":"2026-02-22T11:20:37.891392Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["'CustomData.SpeakerDiarization.train' found in /kaggle/working/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n","âœ“ Database configuration path set\n","\n","Loading dataset protocol...\n","âœ“ Dataset loaded successfully\n","  - Training files: 8\n","  - Development files: 2\n","  - Test files: 14\n","  - Total files: 24\n"]}],"execution_count":2},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport numpy as np\n\n# =============================================================================\n# PYTORCH PATCHES FOR CHECKPOINT LOADING\n# =============================================================================\n# Patch torch.load to allow loading pyannote checkpoints\n# PyAnnote models may contain unpickled objects that require weights_only=False\n\n_orig_load = torch.load\n\ndef _patched_load(f, *args, **kwargs):\n    \"\"\"Patched torch.load that disables weights_only restriction.\"\"\"\n    kwargs['weights_only'] = False\n    return _orig_load(f, *args, **kwargs)\n\ntorch.load = _patched_load\nprint(\"âœ“ PyTorch load function patched for checkpoint compatibility\")\n\n# =============================================================================\n# NUMPY COMPATIBILITY PATCHES\n# =============================================================================\n# Add np.NaN and np.Inf if they don't exist (for older numpy versions)\n\nif not hasattr(np, 'NaN'):\n    np.NaN = np.nan\n    print(\"âœ“ Added np.NaN attribute\")\n\nif not hasattr(np, 'Inf'):\n    np.Inf = np.inf\n    print(\"âœ“ Added np.Inf attribute\")\n\n# =============================================================================\n# GPU VERIFICATION\n# =============================================================================\n# Verify GPU availability (required for training)\n\nassert torch.cuda.is_available(), 'No GPU available! Please enable GPU in Kaggle settings.'\n\ndevice = torch.device('cuda')\nprint(f'\\nâœ“ PyTorch {torch.__version__}')\nprint(f'âœ“ GPU: {torch.cuda.get_device_name(0)}')\nprint(f'âœ“ Device: {device}')","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:20:41.211952Z","iopub.status.busy":"2026-02-22T11:20:41.211197Z","iopub.status.idle":"2026-02-22T11:20:44.154258Z","shell.execute_reply":"2026-02-22T11:20:44.153418Z","shell.execute_reply.started":"2026-02-22T11:20:41.211923Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ“ PyTorch load function patched for checkpoint compatibility\n","âœ“ Added np.NaN attribute\n","âœ“ Added np.Inf attribute\n","\n","âœ“ PyTorch 2.8.0+cu126\n","âœ“ GPU: Tesla T4\n","âœ“ Device: cuda\n"]}],"execution_count":3},{"cell_type":"markdown","source":"## ğŸ¤– Section 4: Load Pretrained Model & Pipeline\n\nLoad the pretrained segmentation model and full diarization pipeline from Hugging Face.","metadata":{}},{"cell_type":"code","source":"from pyannote.audio import Model, Inference, Pipeline\n\n# =============================================================================\n# HUGGING FACE AUTHENTICATION\n# =============================================================================\n# Required for downloading pretrained models from Hugging Face Hub\n# Get your token from: https://huggingface.co/settings/tokens\n\nHF_TOKEN = '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'\n\n# =============================================================================\n# LOAD PRETRAINED SEGMENTATION MODEL\n# =============================================================================\n# This model will be fine-tuned on our custom dataset\n# Using segmentation-3.0 for speaker activity detection\n\nprint(\"Loading pretrained segmentation model...\")\npretrained = Model.from_pretrained(\n    'pyannote/segmentation-3.0',\n    token=HF_TOKEN\n)\nprint(\"âœ“ Pretrained segmentation model loaded\")\n\n# =============================================================================\n# LOAD FULL DIARIZATION PIPELINE\n# =============================================================================\n# This pipeline will be used for evaluation\n# It includes segmentation, embedding, and clustering components\n\nprint(\"\\nLoading pretrained diarization pipeline...\")\npipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-community-1\",\n    token=HF_TOKEN\n)\npipeline.to(device)\nprint(\"âœ“ Pretrained pipeline loaded and moved to GPU\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:20:46.339367Z","iopub.status.busy":"2026-02-22T11:20:46.338533Z","iopub.status.idle":"2026-02-22T11:21:05.584373Z","shell.execute_reply":"2026-02-22T11:21:05.583636Z","shell.execute_reply.started":"2026-02-22T11:20:46.339337Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pretrained segmentation model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ae9265adeff4aa6ba6b94f76b55dd1b","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ“ Pretrained segmentation model loaded\n","\n","Loading pretrained diarization pipeline...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dddd4b8093014e00b829c8296d45729e","version_major":2,"version_minor":0},"text/plain":["config.yaml:   0%|          | 0.00/444 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"907dd0ca06bd4d9f884f38178a4568fe","version_major":2,"version_minor":0},"text/plain":["segmentation/pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e96b1bb9ade40d08e0fdd4b1180223b","version_major":2,"version_minor":0},"text/plain":["plda/xvec_transform.npz:   0%|          | 0.00/134k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f7bd6d5836b48368367383d794b6a0c","version_major":2,"version_minor":0},"text/plain":["plda/plda.npz:   0%|          | 0.00/134k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f047cd7f8f7d472eb1eb5c08bc1d9b89","version_major":2,"version_minor":0},"text/plain":["embedding/pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ“ Pretrained pipeline loaded and moved to GPU\n"]}],"execution_count":4},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## ğŸ¯ Section 5: Create Speaker Diarization Task\n\nConfigure the speaker diarization task with training parameters.","metadata":{}},{"cell_type":"code","source":"from pyannote.audio.tasks import SpeakerDiarization as SpeakerDiarizationTask\n\n# =============================================================================\n# RELOAD PROTOCOL\n# =============================================================================\n# Protocol iterators can only be consumed once, so reload for task creation\n\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# =============================================================================\n# CREATE SPEAKER DIARIZATION TASK\n# =============================================================================\n\nprint(\"Creating speaker diarization task...\")\n\nseg_task = SpeakerDiarizationTask(\n    dataset,\n    duration=10.0,                    # Length of audio chunks (seconds)\n    max_speakers_per_chunk=3,         # Maximum speakers per chunk\n    max_speakers_per_frame=1          # Maximum simultaneous speakers per frame\n)\n\nprint(\"âœ“ SpeakerDiarization task created\")\nprint(f\"  - Chunk duration: 10.0 seconds\")\nprint(f\"  - Max speakers per chunk: 3\")\nprint(f\"  - Max speakers per frame: 1\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:21:08.780713Z","iopub.status.busy":"2026-02-22T11:21:08.780127Z","iopub.status.idle":"2026-02-22T11:21:08.867038Z","shell.execute_reply":"2026-02-22T11:21:08.866470Z","shell.execute_reply.started":"2026-02-22T11:21:08.780680Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating speaker diarization task...\n","âœ“ SpeakerDiarization task created\n","  - Chunk duration: 10.0 seconds\n","  - Max speakers per chunk: 3\n","  - Max speakers per frame: 1\n"]}],"execution_count":5},{"cell_type":"markdown","source":"## ğŸ“Š Section 6: Evaluate Pretrained Pipeline (Baseline)\n\nEstablish baseline performance by evaluating the pretrained model on our dataset.","metadata":{}},{"cell_type":"code","source":"from pyannote.metrics.diarization import DiarizationErrorRate\nfrom pyannote.core import Annotation, Segment\n\n# =============================================================================\n# HELPER FUNCTION: CONVERT TO ANNOTATION\n# =============================================================================\n\ndef to_annotation(pipeline_output):\n    \"\"\"\n    Convert pipeline output to Annotation object.\n    \n    Handles multiple PyAnnote versions:\n    - PyAnnote 3.x: Returns Annotation directly\n    - PyAnnote 4.x: Returns DiarizeOutput with .speaker_diarization attribute\n    - Fallback: Uses itertracks() method\n    \n    Args:\n        pipeline_output: Output from diarization pipeline\n        \n    Returns:\n        Annotation object with speaker segments\n        \n    Raises:\n        TypeError: If output format is not recognized\n    \"\"\"\n    # PyAnnote 3.x: Already an Annotation\n    if isinstance(pipeline_output, Annotation):\n        return pipeline_output\n    \n    # PyAnnote 4.x: DiarizeOutput with .speaker_diarization\n    if hasattr(pipeline_output, 'speaker_diarization'):\n        ann = Annotation()\n        for turn, speaker in pipeline_output.speaker_diarization:\n            ann[Segment(turn.start, turn.end)] = speaker\n        return ann\n    \n    # Fallback: Try itertracks method\n    if hasattr(pipeline_output, 'itertracks'):\n        ann = Annotation()\n        for turn, _, speaker in pipeline_output.itertracks(yield_label=True):\n            ann[Segment(turn.start, turn.end)] = speaker\n        return ann\n    \n    raise TypeError(f\"Cannot convert {type(pipeline_output)} to Annotation\")\n\n# =============================================================================\n# EVALUATE PRETRAINED MODEL\n# =============================================================================\n\nprint(\"Evaluating pretrained pipeline on development set...\")\nprint(\"This may take several minutes...\\n\")\n\n# Initialize Diarization Error Rate metric\nmetric = DiarizationErrorRate()\n\n# Reload protocol for evaluation\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Evaluate on development set\ndev_files_list = list(dataset.development())\nfor i, file in enumerate(dev_files_list, 1):\n    print(f\"  [{i}/{len(dev_files_list)}] Processing: {file['uri']}\")\n    \n    # Run diarization\n    hyp = to_annotation(pipeline(file))\n    \n    # Store hypothesis\n    file['pretrained pipeline'] = hyp\n    \n    # Update metric with ground truth and hypothesis\n    metric(file['annotation'], hyp, uem=file['annotated'])\n\n# Calculate final DER\nder_pretrained = abs(metric)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BASELINE PERFORMANCE (Pretrained Model)\")\nprint(\"=\"*60)\nprint(f\"Diarization Error Rate (DER): {100 * der_pretrained:.1f}%\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:21:11.228849Z","iopub.status.busy":"2026-02-22T11:21:11.228188Z","iopub.status.idle":"2026-02-22T11:34:30.079671Z","shell.execute_reply":"2026-02-22T11:34:30.078838Z","shell.execute_reply.started":"2026-02-22T11:21:11.228815Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating pretrained pipeline on development set...\n","This may take several minutes...\n","\n","  [1/2] Processing: train_011\n","  [2/2] Processing: train_013\n","\n","============================================================\n","BASELINE PERFORMANCE (Pretrained Model)\n","============================================================\n","Diarization Error Rate (DER): 55.0%\n","============================================================\n","\n"]}],"execution_count":6},{"cell_type":"markdown","source":"## ğŸ‹ï¸ Section 7: Fine-Tune the Model (with Checkpointing)\n\nFine-tune the segmentation model on our custom dataset.\nCheckpoints will be saved to the output directory.","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# =============================================================================\n# CLONE MODEL FOR FINE-TUNING\n# =============================================================================\n# Create a deep copy to preserve the original pretrained model\n\nprint(\"Creating model for fine-tuning...\")\nfinetuned = deepcopy(pretrained)\nfinetuned.task = seg_task\nprint(\"âœ“ Model cloned and task assigned\")\n\n# =============================================================================\n# SETUP CHECKPOINT CALLBACK\n# =============================================================================\n# Save checkpoints during training to the output directory\n\ncheckpoint_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nprint(f\"\\nâœ“ Checkpoint directory: {checkpoint_dir}\")\n\n# ModelCheckpoint callback configuration\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=checkpoint_dir,\n    filename='best-model-{epoch:02d}-{loss:.2f}',\n    save_top_k=1,              # Save only the best checkpoint\n    monitor='loss/train',      # Monitor training loss\n    mode='min',                # Save checkpoint with minimum loss\n    save_last=True,            # Also save the last checkpoint\n    verbose=True\n)\n\nprint(\"âœ“ Checkpoint callback configured\")\nprint(f\"  - Monitoring: loss/train\")\nprint(f\"  - Saving best model based on minimum loss\")\nprint(f\"  - Also saving last checkpoint\")\n\n# =============================================================================\n# SETUP TRAINER\n# =============================================================================\n\nprint(\"\\nConfiguring PyTorch Lightning trainer...\")\n\ntrainer = pl.Trainer(\n    devices=1,\n    accelerator=\"gpu\",\n    max_epochs=50,\n    default_root_dir=OUTPUT_DIR,\n    callbacks=[checkpoint_callback],\n    enable_checkpointing=True,\n)\n\nprint(\"âœ“ Trainer configured\")\nprint(f\"  - Max epochs: 50\")\nprint(f\"  - Accelerator: GPU\")\nprint(f\"  - Checkpointing: Enabled\")\n\n# =============================================================================\n# START TRAINING\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING FINE-TUNING\")\nprint(\"=\"*60 + \"\\n\")\n\n# Run training\ntrainer.fit(finetuned)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINE-TUNING COMPLETE\")\nprint(\"=\"*60)\n\n# =============================================================================\n# VERIFY CHECKPOINTS SAVED\n# =============================================================================\n\nif os.path.exists(checkpoint_dir):\n    checkpoints = os.listdir(checkpoint_dir)\n    print(f\"\\nâœ“ Checkpoints saved:\")\n    for ckpt in checkpoints:\n        ckpt_path = os.path.join(checkpoint_dir, ckpt)\n        size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n        print(f\"  - {ckpt} ({size_mb:.1f} MB)\")\n    \n    print(f\"\\nâœ“ Best checkpoint path: {checkpoint_callback.best_model_path}\")\nelse:\n    print(\"\\nâš ï¸  WARNING: No checkpoints directory found!\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T11:34:30.081467Z","iopub.status.busy":"2026-02-22T11:34:30.081182Z","iopub.status.idle":"2026-02-22T13:19:48.761642Z","shell.execute_reply":"2026-02-22T13:19:48.761025Z","shell.execute_reply.started":"2026-02-22T11:34:30.081439Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"]},{"name":"stdout","output_type":"stream","text":["Creating model for fine-tuning...\n","âœ“ Model cloned and task assigned\n","\n","âœ“ Checkpoint directory: /kaggle/working/output/checkpoints\n","âœ“ Checkpoint callback configured\n","  - Monitoring: loss/train\n","  - Saving best model based on minimum loss\n","  - Also saving last checkpoint\n","\n","Configuring PyTorch Lightning trainer...\n","âœ“ Trainer configured\n","  - Max epochs: 50\n","  - Accelerator: GPU\n","  - Checkpointing: Enabled\n","\n","============================================================\n","STARTING FINE-TUNING\n","============================================================\n","\n"]},{"name":"stderr","output_type":"stream","text":["2026-02-22 11:34:32.605817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1771760072.836634     147 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1771760072.914367     147 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1771760073.472928     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1771760073.472955     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1771760073.472958     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1771760073.472960     147 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">       In sizes </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                  Out sizes </span>â”ƒ\n","â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ sincnet           â”‚ SincNet          â”‚ 42.6 K â”‚ train â”‚ 960 M â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 160000] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">               [1, 60, 589] </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ lstm              â”‚ LSTM             â”‚  1.4 M â”‚ train â”‚     0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   [1, 589, 60] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [[1, 589, 256], [[8, 1, </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>â”‚                   â”‚                  â”‚        â”‚       â”‚       â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">        128], [8, 1, 128]]] </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ linear            â”‚ ModuleList       â”‚ 49.4 K â”‚ train â”‚     0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">              ? </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                          ? </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>â”‚ classifier        â”‚ Linear           â”‚    516 â”‚ train â”‚ 603 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 589, 128] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                [1, 589, 4] </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>â”‚ activation        â”‚ LogSoftmax       â”‚      0 â”‚ train â”‚     0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 589, 4] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                [1, 589, 4] </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>â”‚ powerset          â”‚ Powerset         â”‚      0 â”‚ train â”‚     0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">              ? </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                          ? </span>â”‚\n","â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>â”‚ validation_metric â”‚ MetricCollection â”‚      0 â”‚ train â”‚     0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">              ? </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                          ? </span>â”‚\n","â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"],"text/plain":["â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m      In sizes\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m                 Out sizes\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n","â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ sincnet           â”‚ SincNet          â”‚ 42.6 K â”‚ train â”‚ 960 M â”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 160000]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m              [1, 60, 589]\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ lstm              â”‚ LSTM             â”‚  1.4 M â”‚ train â”‚     0 â”‚\u001b[37m \u001b[0m\u001b[37m  [1, 589, 60]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [[1, 589, 256], [[8, 1,\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m   \u001b[0mâ”‚                   â”‚                  â”‚        â”‚       â”‚       â”‚\u001b[37m                \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m       128], [8, 1, 128]]]\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0mâ”‚ linear            â”‚ ModuleList       â”‚ 49.4 K â”‚ train â”‚     0 â”‚\u001b[37m \u001b[0m\u001b[37m             ?\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m                         ?\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0mâ”‚ classifier        â”‚ Linear           â”‚    516 â”‚ train â”‚ 603 K â”‚\u001b[37m \u001b[0m\u001b[37m [1, 589, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m               [1, 589, 4]\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0mâ”‚ activation        â”‚ LogSoftmax       â”‚      0 â”‚ train â”‚     0 â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 589, 4]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m               [1, 589, 4]\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0mâ”‚ powerset          â”‚ Powerset         â”‚      0 â”‚ train â”‚     0 â”‚\u001b[37m \u001b[0m\u001b[37m             ?\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m                         ?\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â”‚\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0mâ”‚ validation_metric â”‚ MetricCollection â”‚      0 â”‚ train â”‚     0 â”‚\u001b[37m \u001b[0m\u001b[37m             ?\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m                         ?\u001b[0m\u001b[37m \u001b[0mâ”‚\n","â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n","<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n","<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n","<span style=\"font-weight: bold\">Modules in train mode</span>: 30                                                                                          \n","<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total FLOPs</span>: 1.0 B                                                                                                 \n","</pre>\n"],"text/plain":["\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n","\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n","\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n","\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n","\u001b[1mModules in train mode\u001b[0m: 30                                                                                          \n","\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n","\u001b[1mTotal FLOPs\u001b[0m: 1.0 B                                                                                                 \n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9682362430da45cea92f8aea6311c10a","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 84: 'loss/train' reached 0.56142 (best 0.56142), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=00-loss=0.00.ckpt' as top 1\n","Epoch 1, global step 168: 'loss/train' reached 0.42281 (best 0.42281), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=01-loss=0.00.ckpt' as top 1\n","Epoch 2, global step 252: 'loss/train' reached 0.40484 (best 0.40484), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=02-loss=0.00.ckpt' as top 1\n","Epoch 3, global step 336: 'loss/train' reached 0.36840 (best 0.36840), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=03-loss=0.00.ckpt' as top 1\n","Epoch 4, global step 420: 'loss/train' was not in top 1\n","Epoch 5, global step 504: 'loss/train' was not in top 1\n","Epoch 6, global step 588: 'loss/train' was not in top 1\n","Epoch 7, global step 672: 'loss/train' reached 0.35157 (best 0.35157), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=07-loss=0.00.ckpt' as top 1\n","Epoch 8, global step 756: 'loss/train' reached 0.34086 (best 0.34086), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=08-loss=0.00.ckpt' as top 1\n","Epoch 9, global step 840: 'loss/train' was not in top 1\n","Epoch 10, global step 924: 'loss/train' reached 0.33329 (best 0.33329), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=10-loss=0.00.ckpt' as top 1\n","Epoch 11, global step 1008: 'loss/train' was not in top 1\n","Epoch 12, global step 1092: 'loss/train' reached 0.33217 (best 0.33217), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=12-loss=0.00.ckpt' as top 1\n","Epoch 13, global step 1176: 'loss/train' reached 0.33050 (best 0.33050), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=13-loss=0.00.ckpt' as top 1\n","Epoch 14, global step 1260: 'loss/train' reached 0.31162 (best 0.31162), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=14-loss=0.00.ckpt' as top 1\n","Epoch 15, global step 1344: 'loss/train' was not in top 1\n","Epoch 16, global step 1428: 'loss/train' was not in top 1\n","Epoch 17, global step 1512: 'loss/train' was not in top 1\n","Epoch 18, global step 1596: 'loss/train' was not in top 1\n","Epoch 19, global step 1680: 'loss/train' was not in top 1\n","Epoch 20, global step 1764: 'loss/train' was not in top 1\n","Epoch 21, global step 1848: 'loss/train' was not in top 1\n","Epoch 22, global step 1932: 'loss/train' was not in top 1\n","Epoch 23, global step 2016: 'loss/train' was not in top 1\n","Epoch 24, global step 2100: 'loss/train' was not in top 1\n","Epoch 25, global step 2184: 'loss/train' was not in top 1\n","Epoch 26, global step 2268: 'loss/train' was not in top 1\n","Epoch 27, global step 2352: 'loss/train' was not in top 1\n","Epoch 28, global step 2436: 'loss/train' was not in top 1\n","Epoch 29, global step 2520: 'loss/train' was not in top 1\n","Epoch 30, global step 2604: 'loss/train' reached 0.30561 (best 0.30561), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=30-loss=0.00.ckpt' as top 1\n","Epoch 31, global step 2688: 'loss/train' was not in top 1\n","Epoch 32, global step 2772: 'loss/train' reached 0.30296 (best 0.30296), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=32-loss=0.00.ckpt' as top 1\n","Epoch 33, global step 2856: 'loss/train' reached 0.29747 (best 0.29747), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=33-loss=0.00.ckpt' as top 1\n","Epoch 34, global step 2940: 'loss/train' was not in top 1\n","Epoch 35, global step 3024: 'loss/train' reached 0.27891 (best 0.27891), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=35-loss=0.00.ckpt' as top 1\n","Epoch 36, global step 3108: 'loss/train' was not in top 1\n","Epoch 37, global step 3192: 'loss/train' was not in top 1\n","Epoch 38, global step 3276: 'loss/train' was not in top 1\n","Epoch 39, global step 3360: 'loss/train' was not in top 1\n","Epoch 40, global step 3444: 'loss/train' was not in top 1\n","Epoch 41, global step 3528: 'loss/train' was not in top 1\n","Epoch 42, global step 3612: 'loss/train' was not in top 1\n","Epoch 43, global step 3696: 'loss/train' was not in top 1\n","Epoch 44, global step 3780: 'loss/train' reached 0.27818 (best 0.27818), saving model to '/kaggle/working/output/checkpoints/best-model-epoch=44-loss=0.00.ckpt' as top 1\n","Epoch 45, global step 3864: 'loss/train' was not in top 1\n","Epoch 46, global step 3948: 'loss/train' was not in top 1\n","Epoch 47, global step 4032: 'loss/train' was not in top 1\n","Epoch 48, global step 4116: 'loss/train' was not in top 1\n","Epoch 49, global step 4200: 'loss/train' was not in top 1\n","`Trainer.fit` stopped: `max_epochs=50` reached.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"],"text/plain":[]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","FINE-TUNING COMPLETE\n","============================================================\n","\n","âœ“ Checkpoints saved:\n","  - best-model-epoch=44-loss=0.00.ckpt (16.9 MB)\n","  - last.ckpt (16.9 MB)\n","\n","âœ“ Best checkpoint path: /kaggle/working/output/checkpoints/best-model-epoch=44-loss=0.00.ckpt\n"]}],"execution_count":7},{"cell_type":"markdown","source":"## ğŸ“ˆ Section 8: Evaluate Fine-Tuned Pipeline\n\nLoad the best checkpoint and evaluate the fine-tuned model's performance.\nCompare with baseline to measure improvement.","metadata":{}},{"cell_type":"code","source":"import glob\n\n# =============================================================================\n# LOAD BEST CHECKPOINT\n# =============================================================================\n\nprint(\"Loading best checkpoint...\")\n\n# Search for all checkpoint files recursively\nckpt_pattern = os.path.join(OUTPUT_DIR, '**', '*.ckpt')\nckpts = glob.glob(ckpt_pattern, recursive=True)\n\nif ckpts:\n    # Prefer \"best\" checkpoint, otherwise use last one\n    best_ckpts = [c for c in ckpts if 'best' in os.path.basename(c).lower()]\n    ckpt_path = best_ckpts[0] if best_ckpts else sorted(ckpts)[-1]\n    \n    print(f\"âœ“ Loading checkpoint: {os.path.basename(ckpt_path)}\")\n    print(f\"  Path: {ckpt_path}\")\n    \n    # Load checkpoint and extract model state\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    finetuned.load_state_dict(checkpoint['state_dict'])\n    \n    print(\"âœ“ Checkpoint loaded successfully\")\nelse:\n    print(\"âš ï¸  No checkpoint found, using model from end of training\")\n\n# Set model to evaluation mode\nfinetuned.eval()\nfinetuned.to(device)\n\n# =============================================================================\n# BUILD FINE-TUNED PIPELINE\n# =============================================================================\n# Replace the segmentation component with our fine-tuned model\n\nprint(\"\\nBuilding fine-tuned pipeline...\")\n\n# Load fresh pipeline (uses pretrained components)\nfinetuned_pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-community-1\",\n    token=HF_TOKEN\n)\n\n# Replace the segmentation Inference object with our fine-tuned model\nfinetuned_pipeline._segmentation = Inference(\n    finetuned,\n    duration=finetuned_pipeline._segmentation.duration,\n    step=finetuned_pipeline._segmentation.step,\n    skip_aggregation=finetuned_pipeline._segmentation.skip_aggregation,\n    batch_size=finetuned_pipeline._segmentation.batch_size,\n)\nfinetuned_pipeline.to(device)\n\nprint(\"âœ“ Fine-tuned pipeline ready (segmentation model swapped)\")\n\n# =============================================================================\n# VERIFY MODEL SWAP\n# =============================================================================\n\nassert finetuned_pipeline._segmentation.model is finetuned, \\\n    \"ERROR: Model swap failed! Pipeline is still using the pretrained model.\"\n\nprint(\"âœ“ Verified: Pipeline is using fine-tuned segmentation model\")\n\n# =============================================================================\n# EVALUATE ON DEVELOPMENT SET\n# =============================================================================\n\nprint(\"\\nEvaluating fine-tuned model on development set...\")\nprint(\"This may take several minutes...\\n\")\n\n# Initialize metric\nmetric = DiarizationErrorRate()\n\n# Reload dataset\ndataset = get_protocol('CustomData.SpeakerDiarization.train', {'audio': get_audio_path})\n\n# Evaluate each file\ndev_files_list = list(dataset.development())\nfor i, file in enumerate(dev_files_list, 1):\n    print(f\"  [{i}/{len(dev_files_list)}] Processing: {file['uri']}\")\n    \n    # Run diarization\n    hyp = to_annotation(finetuned_pipeline(file))\n    \n    # Store hypothesis\n    file['finetuned pipeline'] = hyp\n    \n    # Update metric\n    metric(file['annotation'], hyp, uem=file['annotated'])\n\n# Calculate final DER\nder_finetuned = abs(metric)\n\n# =============================================================================\n# COMPARE RESULTS\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS - COMPARISON\")\nprint(\"=\"*60)\nprint(f\"\\nPretrained Model DER:  {100 * der_pretrained:.1f}%\")\nprint(f\"Fine-tuned Model DER:  {100 * der_finetuned:.1f}%\")\n\n# Calculate improvement metrics\nimprovement = (der_pretrained - der_finetuned) / der_pretrained * 100\nabsolute_improvement = 100 * (der_pretrained - der_finetuned)\n\nif improvement > 0:\n    print(f\"\\nâœ“ Relative Improvement: {improvement:.1f}%\")\n    print(f\"âœ“ Absolute Improvement: {absolute_improvement:.1f} percentage points\")\n    print(f\"\\nğŸ‰ Fine-tuning was successful!\")\nelse:\n    print(f\"\\nâš ï¸  No improvement detected\")\n    print(f\"   DER increased by: {-absolute_improvement:.1f} percentage points\")\n    print(\"\\n   Suggestions:\")\n    print(\"   - Train for more epochs\")\n    print(\"   - Adjust learning rate\")\n    print(\"   - Check data quality\")\n    print(\"   - Verify annotation accuracy\")\n\nprint(\"=\"*60 + \"\\n\")","metadata":{"execution":{"iopub.execute_input":"2026-02-22T13:19:48.763245Z","iopub.status.busy":"2026-02-22T13:19:48.762969Z","iopub.status.idle":"2026-02-22T13:33:09.188619Z","shell.execute_reply":"2026-02-22T13:33:09.187911Z","shell.execute_reply.started":"2026-02-22T13:19:48.763217Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading best checkpoint...\n","âœ“ Loading checkpoint: best-model-epoch=44-loss=0.00.ckpt\n","  Path: /kaggle/working/output/checkpoints/best-model-epoch=44-loss=0.00.ckpt\n","âœ“ Checkpoint loaded successfully\n","\n","Building fine-tuned pipeline...\n","âœ“ Fine-tuned pipeline ready (segmentation model swapped)\n","âœ“ Verified: Pipeline is using fine-tuned segmentation model\n","\n","Evaluating fine-tuned model on development set...\n","This may take several minutes...\n","\n","  [1/2] Processing: train_011\n","  [2/2] Processing: train_013\n","\n","============================================================\n","EVALUATION RESULTS - COMPARISON\n","============================================================\n","\n","Pretrained Model DER:  55.0%\n","Fine-tuned Model DER:  34.3%\n","\n","âœ“ Relative Improvement: 37.6%\n","âœ“ Absolute Improvement: 20.7 percentage points\n","\n","ğŸ‰ Fine-tuning was successful!\n","============================================================\n","\n"]}],"execution_count":8}]}